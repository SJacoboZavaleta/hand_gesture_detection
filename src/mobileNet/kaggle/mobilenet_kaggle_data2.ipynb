{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7d8995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:04.514697Z",
     "iopub.status.busy": "2024-12-07T14:38:04.514389Z",
     "iopub.status.idle": "2024-12-07T14:38:13.831089Z",
     "shell.execute_reply": "2024-12-07T14:38:13.830188Z"
    },
    "papermill": {
     "duration": 9.326175,
     "end_time": "2024-12-07T14:38:13.833122",
     "exception": false,
     "start_time": "2024-12-07T14:38:04.506947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\r\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: thop\r\n",
      "Successfully installed thop-0.1.1.post2209072238\r\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa7ac3",
   "metadata": {
    "papermill": {
     "duration": 0.006476,
     "end_time": "2024-12-07T14:38:13.845561",
     "exception": false,
     "start_time": "2024-12-07T14:38:13.839085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EfficientNet Transfer Learning for Hand Gesture Recognition\n",
    "\n",
    "=======================================================\n",
    "\n",
    "\n",
    "\n",
    "This module implements a transfer learning approach using MobileNet V2 for hand gesture \n",
    "\n",
    "recognition. It fine-tunes a pre-trained EfficientNet model on ASL hand gesture data \n",
    "\n",
    "for improved classification performance.\n",
    "\n",
    "\n",
    "\n",
    "Project Overview\n",
    "\n",
    "--------------\n",
    "\n",
    "Part of the Computer Vision Master's Project at UC3M (Universidad Carlos III de Madrid)  \n",
    "\n",
    "Date: 30/11/2024  \n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "\n",
    "\n",
    "Main Features\n",
    "\n",
    "-----------\n",
    "\n",
    "* Transfer learning implementation using MobileNet V2\n",
    "\n",
    "* A Kaggle dataset and a custom dataset handling for hand gesture images\n",
    "\n",
    "* Fine-tuning capabilities with configurable parameters\n",
    "\n",
    "* Training progress monitoring and visualization\n",
    "\n",
    "* Model performance evaluation\n",
    "\n",
    "* Checkpoint saving and loading\n",
    "\n",
    "* Learning rate scheduling\n",
    "\n",
    "\n",
    "\n",
    "Technical Architecture\n",
    "\n",
    "-------------------\n",
    "\n",
    "1. Model Architecture:\n",
    "\n",
    "   - Base: MobileNet V2 pre-trained on ImageNet\n",
    "\n",
    "   - Modified classifier head for gesture classes\n",
    "\n",
    "   - Frozen feature extraction layers\n",
    "\n",
    "   - Fine-tuned top layers\n",
    "\n",
    "\n",
    "\n",
    "2. Training Pipeline:\n",
    "\n",
    "   - Custom dataset loading and preprocessing\n",
    "\n",
    "   - Data augmentation\n",
    "\n",
    "   - Transfer learning optimization\n",
    "\n",
    "   - Learning rate scheduling\n",
    "\n",
    "   - Model checkpointing\n",
    "\n",
    "\n",
    "\n",
    "3. Evaluation Components:\n",
    "\n",
    "   - Training/validation loss tracking\n",
    "\n",
    "   - Accuracy metrics\n",
    "\n",
    "   - Confusion matrix generation\n",
    "\n",
    "   - Performance visualization\n",
    "\n",
    "\n",
    "\n",
    "Dependencies\n",
    "\n",
    "-----------\n",
    "\n",
    "* PyTorch >= 1.9.0: Deep learning framework\n",
    "\n",
    "* torchvision >= 0.10.0: Vision models and utilities\n",
    "\n",
    "* EfficientNet-PyTorch: Pre-trained models\n",
    "\n",
    "* NumPy >= 1.19.0: Numerical computations\n",
    "\n",
    "* Matplotlib >= 3.3.0: Visualization\n",
    "\n",
    "* PIL: Image processing\n",
    "\n",
    "* tqdm: Progress tracking\n",
    "\n",
    "\n",
    "\n",
    "Input Requirements\n",
    "\n",
    "----------------\n",
    "\n",
    "* Dataset Structure:\n",
    "\n",
    "    - Root directory containing class subdirectories\n",
    "\n",
    "    - Images organized by gesture classes\n",
    "\n",
    "    - Supported formats: JPG, PNG  \n",
    "\n",
    "    - Datasets:\n",
    "\n",
    "        - Dataset 1: Custom dataset created from webcam data \n",
    "\n",
    "        - Dataset 2: [ASL Alphabet data](https://www.kaggle.com/datasets/grassknoted/asl-alphabet/data) from Kaggle\n",
    "\n",
    "        - Dataset 3: Custom dataset based on the combination of datasets 1 and 2. \n",
    "\n",
    "* Training Configuration:\n",
    "\n",
    "    - Batch size\n",
    "\n",
    "    - Learning rate\n",
    "\n",
    "    - Number of epochs\n",
    "\n",
    "    - Device selection (CPU/GPU)\n",
    "\n",
    "\n",
    "\n",
    "Output\n",
    "\n",
    "------\n",
    "\n",
    "* Trained Model:\n",
    "\n",
    "    - Saved model checkpoints\n",
    "\n",
    "    - Best model weights\n",
    "\n",
    "    - Training state\n",
    "\n",
    "* Performance Metrics:\n",
    "\n",
    "    - Training/validation loss curves\n",
    "\n",
    "    - Accuracy plots\n",
    "\n",
    "    - Confusion matrix\n",
    "\n",
    "    - Per-class performance metrics\n",
    "\n",
    "\n",
    "\n",
    "Training Parameters\n",
    "\n",
    "-----------------\n",
    "\n",
    "* BATCH_SIZE: Mini-batch size for training\n",
    "\n",
    "* LEARNING_RATE: Initial learning rate\n",
    "\n",
    "* NUM_EPOCHS: Total training epochs\n",
    "\n",
    "* WEIGHT_DECAY: L2 regularization factor\n",
    "\n",
    "* NUM_CLASSES: Number of gesture classes\n",
    "\n",
    "* CHECKPOINT_DIR: Directory for saving models\n",
    "\n",
    "\n",
    "\n",
    "Model Architecture Details\n",
    "\n",
    "------------------------\n",
    "\n",
    "* Base Model: MobileNet V2\n",
    "\n",
    "* Input Size: 224x224x3\n",
    "\n",
    "* Feature Extraction: Pre-trained weights\n",
    "\n",
    "* Classifier Head: Custom fully connected layers\n",
    "\n",
    "* Output: Softmax probabilities for gestures\n",
    "\n",
    "\n",
    "\n",
    "Training Process\n",
    "\n",
    "--------------\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "   - Image resizing and normalization\n",
    "\n",
    "   - Data augmentation (random transforms)\n",
    "\n",
    "   - Batch creation\n",
    "\n",
    "\n",
    "\n",
    "2. Training Loop:\n",
    "\n",
    "   - Forward pass\n",
    "\n",
    "   - Loss computation\n",
    "\n",
    "   - Backpropagation\n",
    "\n",
    "   - Optimizer step\n",
    "\n",
    "   - Learning rate adjustment\n",
    "\n",
    "\n",
    "\n",
    "3. Validation:\n",
    "\n",
    "   - Model evaluation\n",
    "\n",
    "   - Metric computation\n",
    "\n",
    "   - Best model saving\n",
    "\n",
    "\n",
    "\n",
    "Performance Considerations\n",
    "\n",
    "------------------------\n",
    "\n",
    "* GPU Requirements:\n",
    "\n",
    "    - Recommended: NVIDIA GPU with 6GB+ VRAM\n",
    "\n",
    "    - CUDA support required for GPU training\n",
    "\n",
    "* Training Time:\n",
    "\n",
    "    - Varies with dataset size and epochs\n",
    "\n",
    "    - GPU training significantly faster\n",
    "\n",
    "* Memory Usage:\n",
    "\n",
    "    - Depends on batch size\n",
    "\n",
    "    - Typical range: 4-8GB RAM\n",
    "\n",
    "\n",
    "\n",
    "Notes\n",
    "\n",
    "-----\n",
    "\n",
    "* Pre-trained weights significantly reduce training time\n",
    "\n",
    "* Data augmentation crucial for generalization\n",
    "\n",
    "* Regular checkpointing prevents training loss\n",
    "\n",
    "* Monitor validation metrics for overfitting\n",
    "\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "---------\n",
    "\n",
    "1. EfficientNet Paper: https://arxiv.org/abs/1905.11946\n",
    "\n",
    "2. PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "3. Transfer Learning Guide: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "4. [Add relevant papers or resources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83663ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:13.857671Z",
     "iopub.status.busy": "2024-12-07T14:38:13.857356Z",
     "iopub.status.idle": "2024-12-07T14:38:22.078566Z",
     "shell.execute_reply": "2024-12-07T14:38:22.077639Z"
    },
    "papermill": {
     "duration": 8.229865,
     "end_time": "2024-12-07T14:38:22.080784",
     "exception": false,
     "start_time": "2024-12-07T14:38:13.850919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz\r\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.4.0)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2024.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\r\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Installing collected packages: torchviz\r\n",
      "Successfully installed torchviz-0.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c5fc69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:22.094230Z",
     "iopub.status.busy": "2024-12-07T14:38:22.093969Z",
     "iopub.status.idle": "2024-12-07T14:38:30.286944Z",
     "shell.execute_reply": "2024-12-07T14:38:30.285498Z"
    },
    "papermill": {
     "duration": 8.202979,
     "end_time": "2024-12-07T14:38:30.290130",
     "exception": false,
     "start_time": "2024-12-07T14:38:22.087151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb7e528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:30.308793Z",
     "iopub.status.busy": "2024-12-07T14:38:30.308095Z",
     "iopub.status.idle": "2024-12-07T14:38:38.785012Z",
     "shell.execute_reply": "2024-12-07T14:38:38.784317Z"
    },
    "papermill": {
     "duration": 8.487871,
     "end_time": "2024-12-07T14:38:38.786995",
     "exception": false,
     "start_time": "2024-12-07T14:38:30.299124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import thop\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics #conda install -c conda-forge torchmetrics\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import torchviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d789bb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:38.800622Z",
     "iopub.status.busy": "2024-12-07T14:38:38.800189Z",
     "iopub.status.idle": "2024-12-07T14:38:38.805049Z",
     "shell.execute_reply": "2024-12-07T14:38:38.804219Z"
    },
    "papermill": {
     "duration": 0.013296,
     "end_time": "2024-12-07T14:38:38.806619",
     "exception": false,
     "start_time": "2024-12-07T14:38:38.793323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_TYPE = 'data2'\n",
    "\n",
    "\n",
    "\n",
    "if DATA_TYPE == 'data2':\n",
    "\n",
    "    DATA_PATH = os.path.join('/kaggle/input','asl-alphabet','asl_alphabet_train','asl_alphabet_train')\n",
    "\n",
    "elif DATA_TYPE == 'data3':\n",
    "\n",
    "    DATA_PATH = os.path.join('/kaggle/input','unified-data','unified_data','unified_data','unified_data')\n",
    "\n",
    "else:\n",
    "\n",
    "    raise ValueError(f\"Data {DATA_TYPE} not found.\")\n",
    "\n",
    "\n",
    "\n",
    "# elif DATA_TYPE == 'data1': # No considerado\n",
    "\n",
    "#     DATA_PATH = os.path.join('..', '..', 'data', 'webcam_data','unified_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4f009a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:38:38.819382Z",
     "iopub.status.busy": "2024-12-07T14:38:38.819104Z",
     "iopub.status.idle": "2024-12-07T14:47:20.759503Z",
     "shell.execute_reply": "2024-12-07T14:47:20.758546Z"
    },
    "papermill": {
     "duration": 521.949112,
     "end_time": "2024-12-07T14:47:20.761515",
     "exception": false,
     "start_time": "2024-12-07T14:38:38.812403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando carpetas: 100%|██████████| 29/29 [08:41<00:00, 17.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen del Dataset:\n",
      "Total de imágenes: 87000\n",
      "Número de clases: 29\n",
      "\n",
      "Distribución de clases:\n",
      "Labels\n",
      "A          3000\n",
      "B          3000\n",
      "C          3000\n",
      "D          3000\n",
      "E          3000\n",
      "F          3000\n",
      "G          3000\n",
      "H          3000\n",
      "I          3000\n",
      "J          3000\n",
      "K          3000\n",
      "L          3000\n",
      "M          3000\n",
      "N          3000\n",
      "O          3000\n",
      "P          3000\n",
      "Q          3000\n",
      "R          3000\n",
      "S          3000\n",
      "T          3000\n",
      "U          3000\n",
      "V          3000\n",
      "W          3000\n",
      "X          3000\n",
      "Y          3000\n",
      "Z          3000\n",
      "del        3000\n",
      "nothing    3000\n",
      "space      3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mínimo de muestras por clase: 3000\n",
      "Máximo de muestras por clase: 3000\n",
      "\n",
      "Tamaños de imagen:\n",
      "Mínimo: [200 200]\n",
      "Máximo: [200 200]\n",
      "Moda: [200 200]\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "                                           Filepaths Labels  Label_idx  \\\n",
      "0  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "1  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "2  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "3  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "4  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "\n",
      "   Image_size  \n",
      "0  (200, 200)  \n",
      "1  (200, 200)  \n",
      "2  (200, 200)  \n",
      "3  (200, 200)  \n",
      "4  (200, 200)  \n"
     ]
    }
   ],
   "source": [
    "# Function to create the DataFrame from the dataset\n",
    "\n",
    "# Uncomment to use. The output is a dataframe stored in asl_dataset_info.csv\n",
    "\n",
    "\n",
    "\n",
    "def create_dataframe(data_path):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con las rutas de las imágenes y sus etiquetas.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Ruta al directorio que contiene las carpetas de clases\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columnas ['Filepaths', 'Labels', 'Label_idx']\n",
    "    \"\"\"\n",
    "    # Mapeo de nombres de carpetas a índices\n",
    "    mapeo = {\n",
    "        'A': '0', 'B': '1', 'C': '2', 'D': '3', 'E': '4', 'F': '5', \n",
    "        'G': '6', 'H': '7', 'I': '8', 'J': '9', 'K': '10', 'L': '11', \n",
    "        'M': '12', 'N': '13', 'O': '14', 'P': '15', 'Q': '16', 'R': '17', \n",
    "        'S': '18', 'T': '19', 'U': '20', 'V': '21', 'W': '22', 'X': '23', \n",
    "        'Y': '24', 'Z': '25',\n",
    "        'del': '26', \n",
    "        'nothing': '27', \n",
    "        'space': '28'\n",
    "    }\n",
    "    \n",
    "    # Convertir a Path object para mejor manejo de rutas\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise ValueError(f\"El directorio {data_path} no existe\")\n",
    "    \n",
    "    # Listas para almacenar datos\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    label_indices = []\n",
    "    img_sizes = []\n",
    "    \n",
    "    # Obtener todas las carpetas y ordenarlas según el mapeo\n",
    "    folders = sorted([f for f in data_path.iterdir() if f.is_dir()], \n",
    "                     key=lambda x: mapeo.get(x.name, '-1'))\n",
    "    \n",
    "    print(\"Creando DataFrame...\")\n",
    "    # Usar tqdm para mostrar progreso\n",
    "    for folder in tqdm(folders, desc=\"Procesando carpetas\"):\n",
    "        # Obtener el índice del label desde el mapeo\n",
    "        label_idx = int(mapeo.get(folder.name, '-1'))\n",
    "        \n",
    "        # Verificar que el label es válido\n",
    "        if label_idx == -1:\n",
    "            print(f\"Advertencia: Carpeta {folder.name} no reconocida, saltando...\")\n",
    "            continue\n",
    "        \n",
    "        # Obtener todas las imágenes en la carpeta\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "        images = [f for f in folder.iterdir() \n",
    "                 if f.suffix.lower() in valid_extensions]\n",
    "        \n",
    "        for img_path in images:\n",
    "            # Verificar que la imagen se puede leer\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"Advertencia: No se pudo leer {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                height, width = img.shape[:2]\n",
    "                \n",
    "                filepaths.append(str(img_path))\n",
    "                labels.append(folder.name)\n",
    "                label_indices.append(label_idx)\n",
    "                img_sizes.append((width, height))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {img_path}: {str(e)}\")\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Filepaths': filepaths,\n",
    "        'Labels': labels,\n",
    "        'Label_idx': label_indices,\n",
    "        'Image_size': img_sizes\n",
    "    })\n",
    "    \n",
    "    # Mostrar información del dataset\n",
    "    print(\"\\nResumen del Dataset:\")\n",
    "    print(f\"Total de imágenes: {len(df)}\")\n",
    "    print(f\"Número de clases: {len(df['Labels'].unique())}\")\n",
    "    print(\"\\nDistribución de clases:\")\n",
    "    print(df['Labels'].value_counts().sort_index())\n",
    "    \n",
    "    # Verificar balance de clases\n",
    "    min_samples = df['Labels'].value_counts().min()\n",
    "    max_samples = df['Labels'].value_counts().max()\n",
    "    print(f\"\\nMínimo de muestras por clase: {min_samples}\")\n",
    "    print(f\"Máximo de muestras por clase: {max_samples}\")\n",
    "    \n",
    "    # Verificar tamaños de imagen\n",
    "    sizes = pd.DataFrame(df['Image_size'].tolist(), columns=['width', 'height'])\n",
    "    print(\"\\nTamaños de imagen:\")\n",
    "    print(f\"Mínimo: {sizes.min().values}\")\n",
    "    print(f\"Máximo: {sizes.max().values}\")\n",
    "    print(f\"Moda: {sizes.mode().iloc[0].values}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "try:\n",
    "    # Images in 'data/...'  \n",
    "    df = create_dataframe(DATA_PATH)\n",
    "    \n",
    "    # Save dataframe of images paths and labels\n",
    "    if DATA_TYPE == 'data2':\n",
    "        df.to_csv('asl_dataset_info.csv', index=False)\n",
    "    elif DATA_TYPE == 'data3':\n",
    "        df.to_csv('unified_data_dataset_info.csv', index=False)\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac6e2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:20.778605Z",
     "iopub.status.busy": "2024-12-07T14:47:20.778317Z",
     "iopub.status.idle": "2024-12-07T14:47:20.915151Z",
     "shell.execute_reply": "2024-12-07T14:47:20.914350Z"
    },
    "papermill": {
     "duration": 0.147108,
     "end_time": "2024-12-07T14:47:20.916924",
     "exception": false,
     "start_time": "2024-12-07T14:47:20.769816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Filepaths Labels  Label_idx  \\\n",
      "0  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "1  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "2  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "3  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "4  /kaggle/input/asl-alphabet/asl_alphabet_train/...      A          0   \n",
      "\n",
      "   Image_size  \n",
      "0  (200, 200)  \n",
      "1  (200, 200)  \n",
      "2  (200, 200)  \n",
      "3  (200, 200)  \n",
      "4  (200, 200)  \n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame of images previously created\n",
    "\n",
    "if DATA_TYPE == 'data2':\n",
    "\n",
    "    df = pd.read_csv('/kaggle/working/asl_dataset_info.csv')\n",
    "\n",
    "elif DATA_TYPE == 'data3':\n",
    "\n",
    "    df = pd.read_csv('/kaggle/working/unified_data_dataset_info.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e97a7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:20.934194Z",
     "iopub.status.busy": "2024-12-07T14:47:20.933910Z",
     "iopub.status.idle": "2024-12-07T14:47:21.015677Z",
     "shell.execute_reply": "2024-12-07T14:47:21.014717Z"
    },
    "papermill": {
     "duration": 0.091924,
     "end_time": "2024-12-07T14:47:21.017346",
     "exception": false,
     "start_time": "2024-12-07T14:47:20.925422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla P100-PCIE-16GB\n",
      "GPU memory available: 17.06 GB\n"
     ]
    }
   ],
   "source": [
    "# Configure the device for training\n",
    "\n",
    "def setup_device():\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True  # Optimiza el rendimiento\n",
    "\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "        print(\"GPU not available, using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd9ee13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.033982Z",
     "iopub.status.busy": "2024-12-07T14:47:21.033690Z",
     "iopub.status.idle": "2024-12-07T14:47:21.041705Z",
     "shell.execute_reply": "2024-12-07T14:47:21.040870Z"
    },
    "papermill": {
     "duration": 0.018166,
     "end_time": "2024-12-07T14:47:21.043345",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.025179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Custom Dataset for loading ASL (American Sign Language) images.\n",
    "\n",
    "    \n",
    "\n",
    "    This dataset class handles loading and preprocessing of ASL hand gesture images.\n",
    "\n",
    "    It supports on-the-fly data augmentation and preprocessing for model training.\n",
    "\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        df (pd.DataFrame): DataFrame containing image paths and labels\n",
    "\n",
    "        transform (callable): Torchvision transforms for image preprocessing\n",
    "\n",
    "        is_training (bool): Flag to enable/disable data augmentation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize the ASL Dataset.\n",
    "\n",
    "        \n",
    "\n",
    "        Args:\n",
    "\n",
    "            df (pd.DataFrame): DataFrame with columns ['Filepaths', 'Labels']\n",
    "\n",
    "            transform (callable, optional): Transform to be applied to images\n",
    "\n",
    "            is_training (bool): If True, enables data augmentation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.labels = pd.Categorical(dataframe['Labels']).codes\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        \"\"\"Returns the total number of images in the dataset.\"\"\"\n",
    "\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Fetch and preprocess a single image item from the dataset.\n",
    "\n",
    "        \n",
    "\n",
    "        Args:\n",
    "\n",
    "            idx (int): Index of the image to fetch\n",
    "\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "\n",
    "            tuple: (image, label) where image is the preprocessed tensor\n",
    "\n",
    "                  and label is the corresponding class index\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        img_path = self.dataframe.iloc[idx]['Filepaths']\n",
    "\n",
    "        label = self.dataframe.iloc[idx]['Label_idx']  # Asegúrate de que esto sea un número entero\n",
    "\n",
    "        \n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read and preprocess image\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "            if image is None:\n",
    "\n",
    "                raise ValueError(f\"Image not found: {img_path}\")\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            \n",
    "\n",
    "            if self.transform:\n",
    "\n",
    "                image = self.transform(image)\n",
    "\n",
    "\n",
    "\n",
    "            # Defining data type for labels\n",
    "\n",
    "            label = torch.tensor(int(label), dtype=torch.long)\n",
    "\n",
    "            \n",
    "\n",
    "            return image, label\n",
    "\n",
    "    \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error loading imagen {img_path}: {str(e)}\")\n",
    "\n",
    "            # Retorning a black image\n",
    "\n",
    "            if self.transform:\n",
    "\n",
    "                dummy_image = torch.zeros((3, 224, 224))\n",
    "\n",
    "            else:\n",
    "\n",
    "                dummy_image = np.zeros((224, 224, 3))\n",
    "\n",
    "            return dummy_image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530a2589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.060079Z",
     "iopub.status.busy": "2024-12-07T14:47:21.059427Z",
     "iopub.status.idle": "2024-12-07T14:47:21.070075Z",
     "shell.execute_reply": "2024-12-07T14:47:21.069310Z"
    },
    "papermill": {
     "duration": 0.020745,
     "end_time": "2024-12-07T14:47:21.071632",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.050887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ASLModel(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Custom Neural Network model for ASL gesture recognition using transfer learning.\n",
    "\n",
    "    \n",
    "\n",
    "    This model uses MobileNetV2 as the backbone with custom classification layers.\n",
    "\n",
    "    The architecture is designed to balance accuracy and computational efficiency.\n",
    "\n",
    "    \n",
    "\n",
    "    Architecture Overview:\n",
    "\n",
    "    ---------------------\n",
    "\n",
    "    1. MobileNetV2 backbone (pretrained on ImageNet)\n",
    "\n",
    "    2. Custom dense layers with batch normalization\n",
    "\n",
    "    3. Dropout for regularization\n",
    "\n",
    "    4. Softmax output layer\n",
    "\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        base_model (nn.Module): Pretrained MobileNetV2 model\n",
    "\n",
    "        classifier (nn.Sequential): Custom classification layers\n",
    "\n",
    "        num_classes (int): Number of output classes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, num_classes=29, base_model_name='mobilenet_v2', dense_units=256, dropout_rate=0.5):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize the ASL Model.\n",
    "\n",
    "        \n",
    "\n",
    "        Args:\n",
    "\n",
    "            num_classes (int): Number of output classes (ASL gestures)\n",
    "\n",
    "            base_model_name (str): Name of the pretrained model to use\n",
    "\n",
    "            dense_units (int):\n",
    "\n",
    "            droput_rate (int):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        # Load pretrained model \n",
    "\n",
    "        if base_model_name.lower() == 'mobilenet_v2':\n",
    "\n",
    "            # Load MobileNet V2\n",
    "            self.base_model = models.mobilenet_v2(pretrained=True)\n",
    "            \n",
    "            # Remove the last classification layer\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "            \n",
    "            # Determine the number of features (adjust as needed)\n",
    "            num_features = 1280\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Model {base_model_name} not supported\")\n",
    "\n",
    "        \n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "\n",
    "        # Own classifier\n",
    "\n",
    "        self.dense_block1 = nn.Sequential(\n",
    "\n",
    "            nn.Linear(num_features, dense_units*2, bias=False),\n",
    "\n",
    "            nn.BatchNorm1d(dense_units*2),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(dropout_rate)\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        self.dense_block2 = nn.Sequential(\n",
    "\n",
    "            nn.Linear(dense_units*2, dense_units, bias=False),\n",
    "\n",
    "            nn.BatchNorm1d(dense_units),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.3)\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # Modify classifier\n",
    "\n",
    "        self.classifier = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialize weights\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        \n",
    "\n",
    "        # Freeze pretrained newtork\n",
    "\n",
    "        self.freeze_base_model()\n",
    "\n",
    "    \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Linear):\n",
    "\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "\n",
    "    def freeze_base_model(self):\n",
    "\n",
    "        # Freeze early layers\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "\n",
    "            param.requires_grad = False\n",
    "\n",
    "    \n",
    "\n",
    "    def unfreeze_layers(self, num_layers=30):\n",
    "\n",
    "        trainable_layers = list(self.base_model.parameters())[-num_layers:]\n",
    "\n",
    "        for param in trainable_layers:\n",
    "\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Forward pass through the network.\n",
    "\n",
    "        \n",
    "\n",
    "        Args:\n",
    "\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width)\n",
    "\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "\n",
    "            torch.Tensor: Output predictions of shape (batch_size, num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Base model features\n",
    "\n",
    "        x = self.base_model(x)\n",
    "\n",
    "        \n",
    "\n",
    "        # Global pooling\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "\n",
    "        # Dense blocks\n",
    "\n",
    "        x = self.dense_block1(x)\n",
    "\n",
    "        x = self.dense_block2(x)\n",
    "\n",
    "        \n",
    "\n",
    "        # Output with softmax\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # We don't apply a classification here, we do it later\n",
    "\n",
    "        #out = F.softmax(x, dim=1)\n",
    "\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564f17fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.087835Z",
     "iopub.status.busy": "2024-12-07T14:47:21.087602Z",
     "iopub.status.idle": "2024-12-07T14:47:21.093985Z",
     "shell.execute_reply": "2024-12-07T14:47:21.093127Z"
    },
    "papermill": {
     "duration": 0.016404,
     "end_time": "2024-12-07T14:47:21.095544",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.079140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(df, transform, batch_size=32, train_split=0.8, val_split=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Create train, validation, and test data loaders.\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        df (pd.DataFrame): DataFrame containing image paths and labels\n",
    "\n",
    "        transform (callable): Torchvision transforms for image preprocessing\n",
    "\n",
    "        batch_size (int): Batch size for data loaders\n",
    "\n",
    "        train_split (float): Proportion of data used for training (default: 0.8)\n",
    "\n",
    "        val_split (float): Proportion of data used for validation (default: 0.1)\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = ASLDataset(df, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate sizes\n",
    "\n",
    "    total_size = len(dataset)\n",
    "\n",
    "    train_size = int(train_split * total_size)\n",
    "\n",
    "    val_size = int(val_split * total_size)\n",
    "\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "\n",
    "\n",
    "    # Create splits\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "\n",
    "        dataset,\n",
    "\n",
    "        [train_size, val_size, test_size],\n",
    "\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Adjust workers according to your CPU cores (generally num_cores - 1)\n",
    "\n",
    "    # num_workers = min(4, os.cpu_count() - 1) if os.cpu_count() > 1 else 0\n",
    "\n",
    "\n",
    "\n",
    "    # Configure a common DataLoader for training, validation, and testing dataloaders\n",
    "\n",
    "    dataloader_kwargs = {\n",
    "\n",
    "        'batch_size': batch_size,\n",
    "\n",
    "        'num_workers': 0,#num_workers\n",
    "\n",
    "        'pin_memory': torch.cuda.is_available(),\n",
    "\n",
    "        'persistent_workers': False#if num_workers > 0 else False\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae9cf7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.112599Z",
     "iopub.status.busy": "2024-12-07T14:47:21.112345Z",
     "iopub.status.idle": "2024-12-07T14:47:21.130827Z",
     "shell.execute_reply": "2024-12-07T14:47:21.129976Z"
    },
    "papermill": {
     "duration": 0.029215,
     "end_time": "2024-12-07T14:47:21.132387",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.103172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_evaluation_metrics(model, test_loader, history_phase1, history_phase2, evaluation_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Generate and save comprehensive evaluation metrics for the model.\n",
    "\n",
    "    \n",
    "\n",
    "    This function creates various visualizations and metrics including:\n",
    "\n",
    "    - Training/validation loss curves\n",
    "\n",
    "    - Accuracy plots\n",
    "\n",
    "    - Confusion matrix\n",
    "\n",
    "    - Classification report\n",
    "\n",
    "    - Per-class performance metrics\n",
    "\n",
    "    \n",
    "\n",
    "    Evaluation Components:\n",
    "\n",
    "    ---------------------\n",
    "\n",
    "    1. Model Performance Metrics:\n",
    "\n",
    "        - Test Loss (Cross-Entropy)\n",
    "\n",
    "        - Test Accuracy\n",
    "\n",
    "        - Per-class Precision, Recall, and F1-score\n",
    "\n",
    "    \n",
    "\n",
    "    2. Visualizations:\n",
    "\n",
    "        - Confusion Matrix: Shows prediction patterns across all classes\n",
    "\n",
    "        - Training History Plots:\n",
    "\n",
    "            * Loss curves (training and validation)\n",
    "\n",
    "            * Accuracy curves (training and validation)\n",
    "\n",
    "    \n",
    "\n",
    "    3. Saved Outputs:\n",
    "\n",
    "        - classification_metrics.csv: Detailed per-class metrics\n",
    "\n",
    "        - training_history.json: Complete training history\n",
    "\n",
    "        - confusion_matrix.png: Visual representation of model predictions\n",
    "\n",
    "        - training_curves.png: Learning curves from both training phases\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        model (nn.Module): Trained model to evaluate\n",
    "\n",
    "        test_loader (DataLoader): DataLoader for test data\n",
    "\n",
    "        history_phase1 (dict): Training history from phase 1\n",
    "\n",
    "        history_phase2 (dict): Training history from phase 2\n",
    "\n",
    "        evaluation_path (str): Directory to save evaluation results\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: A dictionary containing all evaluation metrics and history:\n",
    "\n",
    "            {\n",
    "\n",
    "                'training_history': {\n",
    "\n",
    "                    'phase1': {train_losses, train_accuracies, val_losses, val_accuracies},\n",
    "\n",
    "                    'phase2': {train_losses, train_accuracies, val_losses, val_accuracies}\n",
    "\n",
    "                },\n",
    "\n",
    "                'final_metrics': {\n",
    "\n",
    "                    'test_loss': float,\n",
    "\n",
    "                    'test_accuracy': float,\n",
    "\n",
    "                    'classification_report': dict\n",
    "\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate on test set\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluando'):\n",
    "\n",
    "            inputs = inputs.to(DEVICE)\n",
    "\n",
    "            labels = labels.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "            \n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = outputs.float()\n",
    "\n",
    "            \n",
    "\n",
    "            # Loss function using cross entropy \n",
    "\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            \n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            \n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate final metrics \n",
    "\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    \n",
    "\n",
    "    # Load a label mapping \n",
    "\n",
    "    label_mapping = load_label_mapping('/kaggle/input/unified-data/classs_lookup_v2.json')\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate confusion matrix\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    classification_rep = classification_report(all_labels, all_preds, \n",
    "\n",
    "                                            target_names=list(label_mapping.values()),\n",
    "\n",
    "                                            output_dict=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Save visualizations\n",
    "\n",
    "    _save_confusion_matrix(cm, label_mapping, evaluation_path)\n",
    "\n",
    "    _save_training_history(history_phase1, history_phase2, evaluation_path)\n",
    "\n",
    "    \n",
    "\n",
    "    # Save metrics in csv\n",
    "\n",
    "    metrics_df = pd.DataFrame(classification_rep).transpose()\n",
    "\n",
    "    metrics_df.to_csv(os.path.join(evaluation_path, 'classification_metrics.csv'))\n",
    "\n",
    "    \n",
    "\n",
    "    # Prepare complete history\n",
    "\n",
    "    history_data = {\n",
    "\n",
    "        'training_history': {\n",
    "\n",
    "            'phase1': {\n",
    "\n",
    "                'train_losses': [float(x) for x in history_phase1['train_losses']],\n",
    "\n",
    "                'train_accuracies': [float(x) for x in history_phase1['train_accuracies']],\n",
    "\n",
    "                'val_losses': [float(x) for x in history_phase1['val_losses']],\n",
    "\n",
    "                'val_accuracies': [float(x) for x in history_phase1['val_accuracies']]\n",
    "\n",
    "            },\n",
    "\n",
    "            'phase2': {\n",
    "\n",
    "                'train_losses': [float(x) for x in history_phase2['train_losses']],\n",
    "\n",
    "                'train_accuracies': [float(x) for x in history_phase2['train_accuracies']],\n",
    "\n",
    "                'val_losses': [float(x) for x in history_phase2['val_losses']],\n",
    "\n",
    "                'val_accuracies': [float(x) for x in history_phase2['val_accuracies']]\n",
    "\n",
    "            }\n",
    "\n",
    "        },\n",
    "\n",
    "        'final_metrics': {\n",
    "\n",
    "            'test_loss': float(test_loss),\n",
    "\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "\n",
    "            'classification_report': classification_rep\n",
    "\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    # Save history in a JSON\n",
    "\n",
    "    with open(os.path.join(evaluation_path, 'training_history.json'), 'w') as f:\n",
    "\n",
    "        json.dump(history_data, f, indent=4)\n",
    "\n",
    "    \n",
    "\n",
    "    # Show summary\n",
    "\n",
    "    print(\"\\nResumen Final del Entrenamiento:\")\n",
    "\n",
    "    print(f\"Precisión en test: {test_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Pérdida en test: {test_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nMétricas por clase:\")\n",
    "\n",
    "    print(metrics_df)\n",
    "\n",
    "    \n",
    "\n",
    "    return history_data\n",
    "\n",
    "\n",
    "\n",
    "def _save_confusion_matrix(cm, label_mapping, evaluation_path):\n",
    "\n",
    "    \"\"\"Save confusion matrix of model evaluation\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "\n",
    "                xticklabels=list(label_mapping.values()),\n",
    "\n",
    "                yticklabels=list(label_mapping.values()))\n",
    "\n",
    "    plt.title('Matriz de Confusión')\n",
    "\n",
    "    plt.xlabel('Predicción')\n",
    "\n",
    "    plt.ylabel('Valor Real')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.yticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(evaluation_path, 'confusion_matrix.png'))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def _save_training_history(history_phase1, history_phase2, evaluation_path):\n",
    "\n",
    "    \"\"\"Save plots of training history\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    \n",
    "\n",
    "    # Loss plot\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.plot(history_phase1['train_losses'], label='Phase 1 Train')\n",
    "\n",
    "    plt.plot(history_phase1['val_losses'], label='Phase 1 Val')\n",
    "\n",
    "    plt.plot([len(history_phase1['train_losses']) + i for i in range(len(history_phase2['train_losses']))],\n",
    "\n",
    "             history_phase2['train_losses'], label='Phase 2 Train')\n",
    "\n",
    "    plt.plot([len(history_phase1['val_losses']) + i for i in range(len(history_phase2['val_losses']))],\n",
    "\n",
    "             history_phase2['val_losses'], label='Phase 2 Val')\n",
    "\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "\n",
    "    # Accuracy plot\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.plot(history_phase1['train_accuracies'], label='Phase 1 Train')\n",
    "\n",
    "    plt.plot(history_phase1['val_accuracies'], label='Phase 1 Val')\n",
    "\n",
    "    plt.plot([len(history_phase1['train_accuracies']) + i for i in range(len(history_phase2['train_accuracies']))],\n",
    "\n",
    "             history_phase2['train_accuracies'], label='Phase 2 Train')\n",
    "\n",
    "    plt.plot([len(history_phase1['val_accuracies']) + i for i in range(len(history_phase2['val_accuracies']))],\n",
    "\n",
    "             history_phase2['val_accuracies'], label='Phase 2 Val')\n",
    "\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(evaluation_path, 'training_history.png'))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5925313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.148543Z",
     "iopub.status.busy": "2024-12-07T14:47:21.148289Z",
     "iopub.status.idle": "2024-12-07T14:47:21.162824Z",
     "shell.execute_reply": "2024-12-07T14:47:21.162037Z"
    },
    "papermill": {
     "duration": 0.024446,
     "end_time": "2024-12-07T14:47:21.164365",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.139919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_phase(model, train_loader, val_loader, optimizer, num_epochs, phase_name, early_stopping_patience, output_dir):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Train the model for a single phase.\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        model (nn.Module): The model to train.\n",
    "\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "\n",
    "        val_loader (DataLoader): DataLoader for validation data.    \n",
    "\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "        phase_name (str): Name of the training phase.\n",
    "\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.    \n",
    "\n",
    "        output_dir (str): Directory to save model checkpoints and logs.\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: A dictionary containing training history.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    train_accuracies = []\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    val_accuracies = []\n",
    "\n",
    "    \n",
    "\n",
    "    # Create output directory\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_dir, 'checkpoints')\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda') # For precised training\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        \n",
    "\n",
    "        # Training mode\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        train_steps = 0\n",
    "\n",
    "        train_correct = 0 \n",
    "\n",
    "        train_total = 0 \n",
    "\n",
    "        \n",
    "\n",
    "        # Progress bar during training\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'{phase_name} Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "\n",
    "        \n",
    "\n",
    "        for inputs, labels in train_pbar:\n",
    "\n",
    "            inputs = inputs.to(DEVICE)\n",
    "\n",
    "            labels = labels.to(DEVICE,dtype=torch.long)\n",
    "\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "\n",
    "            # Training with a mixed precision\n",
    "\n",
    "            with torch.cuda.amp.autocast(): #torch.amp.autocast('cuda')\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                outputs = outputs.float()\n",
    "\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            \n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            scaler.update()\n",
    "\n",
    "            \n",
    "\n",
    "            # Calculate accuracy\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "            # Calcute loss\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            train_steps += 1\n",
    "\n",
    "            \n",
    "\n",
    "             # Calcute current accuracy\n",
    "\n",
    "            current_train_acc = 100 * train_correct / train_total\n",
    "\n",
    "\n",
    "\n",
    "            # Update progress bar\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "\n",
    "                'acc': f'{current_train_acc:.2f}%'\n",
    "\n",
    "            })\n",
    "\n",
    "        \n",
    "\n",
    "        # Calcute training metrics\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "\n",
    "\n",
    "        # Save metrics\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluation mode\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_val_loss = 0\n",
    "\n",
    "        val_steps = 0\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        \n",
    "\n",
    "        # Progress bar during validation\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f'{phase_name} Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for inputs, labels in val_pbar:\n",
    "\n",
    "                inputs = inputs.to(DEVICE)\n",
    "\n",
    "                labels = labels.to(DEVICE,dtype=torch.long)\n",
    "\n",
    "                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                outputs = outputs.float()\n",
    "\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                \n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                val_steps += 1\n",
    "\n",
    "                \n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                \n",
    "\n",
    "                # Update progress bar\n",
    "\n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        \n",
    "\n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        checkpoint = {\n",
    "\n",
    "            'epoch': epoch,\n",
    "\n",
    "            'model_state_dict': model.state_dict(),\n",
    "\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "            'train_loss': avg_train_loss,\n",
    "\n",
    "            'train_accuracy': train_accuracy,  \n",
    "\n",
    "            'val_loss': avg_val_loss,\n",
    "\n",
    "            'val_accuracy': val_accuracy\n",
    "\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        # Early stopping\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "            patience_counter = 0\n",
    "\n",
    "            \n",
    "\n",
    "            # Save best model\n",
    "\n",
    "            best_model_path  = os.path.join(checkpoint_dir, f'{phase_name}_best_model.pth')\n",
    "\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "\n",
    "            print(f\"\\nGuardando el mejor modelo en {best_model_path}\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Show metrics\n",
    "\n",
    "        print(f'\\n{phase_name} Epoch {epoch+1}/{num_epochs}:')\n",
    "\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "\n",
    "            print(f'\\nEarly stopping triggered after {patience_counter} epochs without improvement')\n",
    "\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "    # Load the best model before returing\n",
    "\n",
    "    best_checkpoint = torch.load(best_model_path)\n",
    "\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "\n",
    "        'train_losses': train_losses,\n",
    "\n",
    "        'train_accuracies': train_accuracies,\n",
    "\n",
    "        'val_losses': val_losses,\n",
    "\n",
    "        'val_accuracies': val_accuracies\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c13689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.180538Z",
     "iopub.status.busy": "2024-12-07T14:47:21.180302Z",
     "iopub.status.idle": "2024-12-07T14:47:21.188452Z",
     "shell.execute_reply": "2024-12-07T14:47:21.187726Z"
    },
    "papermill": {
     "duration": 0.018009,
     "end_time": "2024-12-07T14:47:21.189937",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.171928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_complete(df, transform, num_classes=29, batch_size=32, num_epochs1=5, num_epochs2=5, \n",
    "\n",
    "                        learning_rate1=1e-3, learning_rate2=1e-5, early_stopping_patience=3, output_dir='results/'):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Train the model in two phases: Transfer Learning and Fine-tuning.\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        df (pd.DataFrame): DataFrame containing the training data.\n",
    "\n",
    "        transform (torchvision.transforms.Compose): Transformations to apply to the input data.\n",
    "\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "        num_epochs1 (int): Number of epochs for the first phase of training.\n",
    "\n",
    "        num_epochs2 (int): Number of epochs for the second phase of training.\n",
    "\n",
    "        learning_rate1 (float): Learning rate for the first phase of training.\n",
    "\n",
    "        learning_rate2 (float): Learning rate for the second phase of training.\n",
    "\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "\n",
    "        output_dir (str): Directory to save model checkpoints and logs.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        tuple: (trained_model, training_history)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Starting the complete training...\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Crear data loaders\n",
    "\n",
    "    print(\"Creating  data loaders...\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "\n",
    "        df=df,\n",
    "\n",
    "        transform=transform,\n",
    "\n",
    "        batch_size=batch_size\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Create the model and move it to device\n",
    "\n",
    "    print(\"Initializing the model...\")\n",
    "\n",
    "    model = ASLModel(num_classes=num_classes)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "        \n",
    "\n",
    "    # Fase 1: Transfer Learning\n",
    "\n",
    "    print(\"\\n Phase 1:  Transfer Learning - Only new layers\")\n",
    "\n",
    "    optimizer_phase1 = torch.optim.AdamW(\n",
    "\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "\n",
    "        lr=learning_rate1,\n",
    "\n",
    "        weight_decay=1e-4\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    history_phase1 = train_one_phase(\n",
    "\n",
    "        model=model,\n",
    "\n",
    "        train_loader=train_loader,\n",
    "\n",
    "        val_loader=val_loader,\n",
    "\n",
    "        optimizer=optimizer_phase1,\n",
    "\n",
    "        num_epochs=num_epochs1,\n",
    "\n",
    "        phase_name='Transfer_Learning',\n",
    "\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "\n",
    "        output_dir=output_dir\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Fase 2: Fine-tuning\n",
    "\n",
    "    print(\"\\n Phase 2: Fine-tuning - Complete model\")\n",
    "\n",
    "    model.unfreeze_layers(num_layers=30)\n",
    "\n",
    "    \n",
    "\n",
    "    optimizer_phase2 = torch.optim.AdamW(\n",
    "\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "\n",
    "        lr=learning_rate2,\n",
    "\n",
    "        weight_decay=1e-4\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    history_phase2 = train_one_phase(\n",
    "\n",
    "        model=model,\n",
    "\n",
    "        train_loader=train_loader,\n",
    "\n",
    "        val_loader=val_loader,\n",
    "\n",
    "        optimizer=optimizer_phase2,\n",
    "\n",
    "        num_epochs=num_epochs2,\n",
    "\n",
    "        phase_name='Fine_Tuning',\n",
    "\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "\n",
    "        output_dir=output_dir\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a new directory to save evaluation\n",
    "\n",
    "    evaluation_path = os.path.join(output_dir, 'evaluation')\n",
    "\n",
    "    os.makedirs(evaluation_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Evaluate model\n",
    "\n",
    "    print(\"\\nRealizando evaluación final...\")\n",
    "\n",
    "    history_data = generate_evaluation_metrics(\n",
    "\n",
    "        model=model,\n",
    "\n",
    "        test_loader=test_loader,\n",
    "\n",
    "        history_phase1=history_phase1,\n",
    "\n",
    "        history_phase2=history_phase2,\n",
    "\n",
    "        evaluation_path=evaluation_path\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"\\nResultados guardados en: {evaluation_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "    return model, history_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ae9aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.206205Z",
     "iopub.status.busy": "2024-12-07T14:47:21.205743Z",
     "iopub.status.idle": "2024-12-07T14:47:21.211462Z",
     "shell.execute_reply": "2024-12-07T14:47:21.210662Z"
    },
    "papermill": {
     "duration": 0.015658,
     "end_time": "2024-12-07T14:47:21.212982",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.197324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_label_mapping(json_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Load label mapping from a JSON file.\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        json_path (str): Path to the JSON file.\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: A dictionary mapping class indices to class labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "\n",
    "            class_mapping = json.load(f)\n",
    "\n",
    "        \n",
    "\n",
    "        # Convertir las claves de string a int y los valores a mayúsculas\n",
    "\n",
    "        label_mapping = {int(k): v.upper() for k, v in class_mapping.items()}\n",
    "\n",
    "        return label_mapping\n",
    "\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error by loading label mapping: {e}\")\n",
    "\n",
    "        print(\"Using default label mapping...\")\n",
    "\n",
    "        \n",
    "\n",
    "        # Mapeo por defecto\n",
    "\n",
    "        default_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', \n",
    "\n",
    "                         'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n",
    "\n",
    "                         'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "        label_mapping = {idx: label for idx, label in enumerate(default_labels)}\n",
    "\n",
    "        return label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c587ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.229494Z",
     "iopub.status.busy": "2024-12-07T14:47:21.228799Z",
     "iopub.status.idle": "2024-12-07T14:47:21.240217Z",
     "shell.execute_reply": "2024-12-07T14:47:21.239422Z"
    },
    "papermill": {
     "duration": 0.021358,
     "end_time": "2024-12-07T14:47:21.241781",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.220423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model_architecture(model, output_dir, input_size=(3, 224, 224)):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Save a visual representation of the model architecture.\n",
    "\n",
    "    \n",
    "\n",
    "    Creates a simplified visualization of the model's architecture using graphviz,\n",
    "\n",
    "    showing the main components and their connections.\n",
    "\n",
    "    \n",
    "\n",
    "    Args:\n",
    "\n",
    "        model (nn.Module): Model to visualize\n",
    "\n",
    "        output_dir (str): Directory to save the visualization\n",
    "\n",
    "        input_size (tuple): Input tensor dimensions (channels, height, width)\n",
    "\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        str: Path to the saved visualization and summary files\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "\n",
    "    from contextlib import redirect_stdout\n",
    "\n",
    "    from torchinfo import summary\n",
    "\n",
    "    from graphviz import Digraph\n",
    "\n",
    "    \n",
    "\n",
    "    # Create output directory\n",
    "\n",
    "    architecture_dir = os.path.join(output_dir, 'model_architecture')\n",
    "\n",
    "    os.makedirs(architecture_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        # Create graph\n",
    "\n",
    "        dot = Digraph(comment='Model Architecture')\n",
    "\n",
    "        dot.attr(rankdir='TB')\n",
    "\n",
    "        dot.attr('node', shape='box', style='rounded')\n",
    "\n",
    "        \n",
    "\n",
    "        # Define style to nodes\n",
    "\n",
    "        dot.attr('node', fontname='Arial')\n",
    "\n",
    "        \n",
    "\n",
    "        # Input\n",
    "\n",
    "        channels, height, width = input_size\n",
    "\n",
    "        input_label = f'Input\\n({height}×{width}×{channels})'\n",
    "\n",
    "        dot.node('input', input_label, shape='oval')\n",
    "\n",
    "        \n",
    "\n",
    "        # MobileNetV2 (Pre-trained)\n",
    "\n",
    "        dot.node('backbone', 'MobileNetV2\\n(Pre-trained)', style='filled', fillcolor='lightgray')\n",
    "\n",
    "        \n",
    "\n",
    "        # Global Pooling\n",
    "\n",
    "        dot.node('pool', 'Global Average Pooling')\n",
    "\n",
    "        \n",
    "\n",
    "        # Dense blocks\n",
    "\n",
    "        dot.node('dense1', 'Dense Block 1\\n512 units\\nBatchNorm + ReLU\\nDropout (0.5)')\n",
    "\n",
    "        dot.node('dense2', 'Dense Block 2\\n256 units\\nBatchNorm + ReLU\\nDropout (0.3)')\n",
    "\n",
    "        \n",
    "\n",
    "        # Output\n",
    "\n",
    "        dot.node('output', 'Output Layer\\n29 classes\\nSoftmax', shape='oval')\n",
    "\n",
    "        \n",
    "\n",
    "        # Add conexions \n",
    "\n",
    "        dot.edge('input', 'backbone')\n",
    "\n",
    "        dot.edge('backbone', 'pool')\n",
    "\n",
    "        dot.edge('pool', 'dense1')\n",
    "\n",
    "        dot.edge('dense1', 'dense2')\n",
    "\n",
    "        dot.edge('dense2', 'output')\n",
    "\n",
    "        \n",
    "\n",
    "        # Save the graph\n",
    "\n",
    "        dot.render(os.path.join(architecture_dir, 'model_architecture_simplified'), \n",
    "\n",
    "                  format='png', cleanup=True)\n",
    "\n",
    "        dot.render(os.path.join(architecture_dir, 'model_architecture_simplified'), \n",
    "\n",
    "                  format='pdf', cleanup=True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Save basic summary\n",
    "\n",
    "        summary_file = os.path.join(architecture_dir, 'model_summary_simplified.txt')\n",
    "\n",
    "        with open(summary_file, 'w') as f:\n",
    "\n",
    "            f.write(\"ASL Hand Gesture Classification Model\\n\")\n",
    "\n",
    "            f.write(\"====================================\\n\\n\")\n",
    "\n",
    "            f.write(\"Architecture Overview:\\n\")\n",
    "\n",
    "            f.write(\"1. Input Layer: 224×224×3\\n\")\n",
    "\n",
    "            f.write(\"2. Backbone: MobileNetV2 (pre-trained)\\n\")\n",
    "\n",
    "            f.write(\"3. Global Average Pooling\\n\")\n",
    "\n",
    "            f.write(\"4. Dense Block 1:\\n\")\n",
    "\n",
    "            f.write(\"   - 512 units\\n\")\n",
    "\n",
    "            f.write(\"   - Batch Normalization\\n\")\n",
    "\n",
    "            f.write(\"   - ReLU Activation\\n\")\n",
    "\n",
    "            f.write(\"   - Dropout (0.5)\\n\")\n",
    "\n",
    "            f.write(\"5. Dense Block 2:\\n\")\n",
    "\n",
    "            f.write(\"   - 256 units\\n\")\n",
    "\n",
    "            f.write(\"   - Batch Normalization\\n\")\n",
    "\n",
    "            f.write(\"   - ReLU Activation\\n\")\n",
    "\n",
    "            f.write(\"   - Dropout (0.3)\\n\")\n",
    "\n",
    "            f.write(\"6. Output Layer:\\n\")\n",
    "\n",
    "            f.write(\"   - 29 units (classes)\\n\")\n",
    "\n",
    "            f.write(\"   - Softmax activation\\n\\n\")\n",
    "\n",
    "            f.write(\"Training Strategy:\\n\")\n",
    "\n",
    "            f.write(\"- Phase 1: Transfer Learning (frozen backbone)\\n\")\n",
    "\n",
    "            f.write(\"- Phase 2: Fine-tuning (last 30 layers unfrozen)\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        print(f\"Visualization saved in  {architecture_dir}\")\n",
    "\n",
    "        print(f\"Summary saved in {summary_file}\")\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error creating the visualization: {str(e)}\")\n",
    "\n",
    "        print(\"Make sure to have graphviz installed:\")\n",
    "\n",
    "        print(\"1. Install graphviz on your system:\")\n",
    "\n",
    "        print(\"   - Windows: https://graphviz.org/download/\")\n",
    "\n",
    "        print(\"   - Linux: sudo apt-get install graphviz\")\n",
    "\n",
    "        print(\"   - macOS: brew install graphviz\")\n",
    "\n",
    "        print(\"2. Install the Python package: pip install graphviz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87d7e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.257674Z",
     "iopub.status.busy": "2024-12-07T14:47:21.257432Z",
     "iopub.status.idle": "2024-12-07T14:47:21.262294Z",
     "shell.execute_reply": "2024-12-07T14:47:21.261495Z"
    },
    "papermill": {
     "duration": 0.014548,
     "end_time": "2024-12-07T14:47:21.263849",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.249301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIME_STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "BASE_MODEL = 'mobilenet_v2'\n",
    "combined_name = BASE_MODEL + '_' + DATA_TYPE\n",
    "# i.e. 'results/mobilenet_v2_data2/evaluation_20241129_114615'\n",
    "\n",
    "if DATA_TYPE == 'data2':\n",
    "\n",
    "    OUTPUT_PATH =  os.path.join('results', combined_name, f'evaluation_{TIME_STAMP}')\n",
    "\n",
    "elif DATA_TYPE == 'data3':\n",
    "\n",
    "    OUTPUT_PATH =  os.path.join('results', combined_name, f'evaluation_{TIME_STAMP}')\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33ca67b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.279626Z",
     "iopub.status.busy": "2024-12-07T14:47:21.279400Z",
     "iopub.status.idle": "2024-12-07T14:47:21.283483Z",
     "shell.execute_reply": "2024-12-07T14:47:21.282686Z"
    },
    "papermill": {
     "duration": 0.013887,
     "end_time": "2024-12-07T14:47:21.285116",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.271229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "PARAMS = {\n",
    "\n",
    "        'input_size': (224, 224, 3),\n",
    "\n",
    "        'num_classes': 29,\n",
    "\n",
    "        'batch_size': 32,\n",
    "\n",
    "        'dense_units': 256,\n",
    "\n",
    "        'dropout_rate': 0.5,\n",
    "\n",
    "        'learning_rate1': 1e-3,\n",
    "\n",
    "        'learning_rate2': 1e-5,\n",
    "\n",
    "        'epochs_phase1': 20,\n",
    "\n",
    "        'epochs_phase2': 20,\n",
    "\n",
    "        'early_stopping_patience': 3\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a6fc56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:21.301414Z",
     "iopub.status.busy": "2024-12-07T14:47:21.300895Z",
     "iopub.status.idle": "2024-12-07T14:47:22.842864Z",
     "shell.execute_reply": "2024-12-07T14:47:22.841922Z"
    },
    "papermill": {
     "duration": 1.552153,
     "end_time": "2024-12-07T14:47:22.844952",
     "exception": false,
     "start_time": "2024-12-07T14:47:21.292799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 186MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157           [-1, 1280, 1, 1]               0\n",
      "          Linear-158                  [-1, 512]         655,360\n",
      "     BatchNorm1d-159                  [-1, 512]           1,024\n",
      "            ReLU-160                  [-1, 512]               0\n",
      "         Dropout-161                  [-1, 512]               0\n",
      "          Linear-162                  [-1, 256]         131,072\n",
      "     BatchNorm1d-163                  [-1, 256]             512\n",
      "            ReLU-164                  [-1, 256]               0\n",
      "         Dropout-165                  [-1, 256]               0\n",
      "          Linear-166                   [-1, 29]           7,453\n",
      "================================================================\n",
      "Total params: 3,019,293\n",
      "Trainable params: 795,421\n",
      "Non-trainable params: 2,223,872\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.88\n",
      "Params size (MB): 11.52\n",
      "Estimated Total Size (MB): 164.98\n",
      "----------------------------------------------------------------\n",
      "Visualization saved in  results/mobilenet_v2_data2/evaluation_20241207_144721/model_architecture\n",
      "Summary saved in results/mobilenet_v2_data2/evaluation_20241207_144721/model_architecture/model_summary_simplified.txt\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "model = ASLModel(num_classes=29, base_model_name=BASE_MODEL)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(PARAMS['input_size'][2], PARAMS['input_size'][0], PARAMS['input_size'][1]), device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# Save simplified model architecture\n",
    "\n",
    "save_model_architecture(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    output_dir=OUTPUT_PATH,\n",
    "\n",
    "    input_size=PARAMS['input_size']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24ab17ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:22.869966Z",
     "iopub.status.busy": "2024-12-07T14:47:22.869092Z",
     "iopub.status.idle": "2024-12-07T14:47:22.874646Z",
     "shell.execute_reply": "2024-12-07T14:47:22.873894Z"
    },
    "papermill": {
     "duration": 0.016669,
     "end_time": "2024-12-07T14:47:22.876702",
     "exception": false,
     "start_time": "2024-12-07T14:47:22.860033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "\n",
    "# Using Imagenet mean and std\n",
    "\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    transforms.Resize((PARAMS['input_size'][0], PARAMS['input_size'][1])),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "\n",
    "    transforms.RandomRotation(15),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c0a1f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T14:47:22.900629Z",
     "iopub.status.busy": "2024-12-07T14:47:22.899925Z",
     "iopub.status.idle": "2024-12-07T17:47:27.424472Z",
     "shell.execute_reply": "2024-12-07T17:47:27.423313Z"
    },
    "papermill": {
     "duration": 10804.535881,
     "end_time": "2024-12-07T17:47:27.426490",
     "exception": false,
     "start_time": "2024-12-07T14:47:22.890609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the complete training...\n",
      "Creating  data loaders...\n",
      "Initializing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Phase 1:  Transfer Learning - Only new layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 1/20 [Train]:   0%|          | 0/2175 [00:00<?, ?it/s]/tmp/ipykernel_23/582377012.py:101: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(): #torch.amp.autocast('cuda')\n",
      "Transfer_Learning Epoch 1/20 [Train]: 100%|██████████| 2175/2175 [04:58<00:00,  7.28it/s, loss=0.3640, acc=75.91%]\n",
      "Transfer_Learning Epoch 1/20 [Val]: 100%|██████████| 272/272 [00:38<00:00,  7.06it/s, loss=0.2018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 1/20:\n",
      "Training Loss: 0.7664, Training Accuracy: 75.91%\n",
      "Validation Loss: 0.2119, Validation Accuracy: 93.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 2/20 [Train]: 100%|██████████| 2175/2175 [04:50<00:00,  7.50it/s, loss=0.4384, acc=86.18%]\n",
      "Transfer_Learning Epoch 2/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.67it/s, loss=0.1206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 2/20:\n",
      "Training Loss: 0.4132, Training Accuracy: 86.18%\n",
      "Validation Loss: 0.1478, Validation Accuracy: 95.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 3/20 [Train]: 100%|██████████| 2175/2175 [04:50<00:00,  7.48it/s, loss=0.2216, acc=88.37%]\n",
      "Transfer_Learning Epoch 3/20 [Val]: 100%|██████████| 272/272 [00:37<00:00,  7.35it/s, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 3/20:\n",
      "Training Loss: 0.3476, Training Accuracy: 88.37%\n",
      "Validation Loss: 0.1201, Validation Accuracy: 96.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 4/20 [Train]: 100%|██████████| 2175/2175 [04:52<00:00,  7.44it/s, loss=0.1192, acc=89.48%]\n",
      "Transfer_Learning Epoch 4/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.62it/s, loss=0.1244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 4/20:\n",
      "Training Loss: 0.3078, Training Accuracy: 89.48%\n",
      "Validation Loss: 0.1042, Validation Accuracy: 96.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 5/20 [Train]: 100%|██████████| 2175/2175 [04:48<00:00,  7.53it/s, loss=0.1722, acc=90.68%]\n",
      "Transfer_Learning Epoch 5/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.54it/s, loss=0.3484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 5/20:\n",
      "Training Loss: 0.2776, Training Accuracy: 90.68%\n",
      "Validation Loss: 0.0925, Validation Accuracy: 97.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 6/20 [Train]: 100%|██████████| 2175/2175 [04:55<00:00,  7.36it/s, loss=0.2120, acc=91.21%]\n",
      "Transfer_Learning Epoch 6/20 [Val]: 100%|██████████| 272/272 [00:40<00:00,  6.68it/s, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 6/20:\n",
      "Training Loss: 0.2597, Training Accuracy: 91.21%\n",
      "Validation Loss: 0.0848, Validation Accuracy: 97.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 7/20 [Train]: 100%|██████████| 2175/2175 [05:19<00:00,  6.80it/s, loss=0.2702, acc=91.75%]\n",
      "Transfer_Learning Epoch 7/20 [Val]: 100%|██████████| 272/272 [00:34<00:00,  7.80it/s, loss=0.0394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 7/20:\n",
      "Training Loss: 0.2460, Training Accuracy: 91.75%\n",
      "Validation Loss: 0.0815, Validation Accuracy: 97.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 8/20 [Train]: 100%|██████████| 2175/2175 [04:59<00:00,  7.25it/s, loss=0.2201, acc=92.41%]\n",
      "Transfer_Learning Epoch 8/20 [Val]: 100%|██████████| 272/272 [00:38<00:00,  7.13it/s, loss=0.0440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 8/20:\n",
      "Training Loss: 0.2270, Training Accuracy: 92.41%\n",
      "Validation Loss: 0.0686, Validation Accuracy: 97.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 9/20 [Train]: 100%|██████████| 2175/2175 [04:51<00:00,  7.45it/s, loss=0.1517, acc=92.63%]\n",
      "Transfer_Learning Epoch 9/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.73it/s, loss=0.1818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 9/20:\n",
      "Training Loss: 0.2204, Training Accuracy: 92.63%\n",
      "Validation Loss: 0.0648, Validation Accuracy: 98.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 10/20 [Train]: 100%|██████████| 2175/2175 [04:48<00:00,  7.55it/s, loss=0.1874, acc=92.94%]\n",
      "Transfer_Learning Epoch 10/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.57it/s, loss=0.0586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 10/20:\n",
      "Training Loss: 0.2113, Training Accuracy: 92.94%\n",
      "Validation Loss: 0.0576, Validation Accuracy: 98.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 11/20 [Train]: 100%|██████████| 2175/2175 [04:48<00:00,  7.54it/s, loss=0.4542, acc=93.11%]\n",
      "Transfer_Learning Epoch 11/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.62it/s, loss=0.0594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 11/20:\n",
      "Training Loss: 0.2069, Training Accuracy: 93.11%\n",
      "Validation Loss: 0.0570, Validation Accuracy: 98.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 12/20 [Train]: 100%|██████████| 2175/2175 [04:50<00:00,  7.50it/s, loss=0.0751, acc=93.49%]\n",
      "Transfer_Learning Epoch 12/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.69it/s, loss=0.1282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Transfer_Learning_best_model.pth\n",
      "\n",
      "Transfer_Learning Epoch 12/20:\n",
      "Training Loss: 0.1955, Training Accuracy: 93.49%\n",
      "Validation Loss: 0.0499, Validation Accuracy: 98.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 13/20 [Train]: 100%|██████████| 2175/2175 [05:00<00:00,  7.23it/s, loss=0.1410, acc=93.54%]\n",
      "Transfer_Learning Epoch 13/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.57it/s, loss=0.0352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfer_Learning Epoch 13/20:\n",
      "Training Loss: 0.1922, Training Accuracy: 93.54%\n",
      "Validation Loss: 0.0546, Validation Accuracy: 98.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 14/20 [Train]: 100%|██████████| 2175/2175 [04:55<00:00,  7.36it/s, loss=0.1763, acc=93.83%]\n",
      "Transfer_Learning Epoch 14/20 [Val]: 100%|██████████| 272/272 [00:37<00:00,  7.24it/s, loss=0.0682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfer_Learning Epoch 14/20:\n",
      "Training Loss: 0.1831, Training Accuracy: 93.83%\n",
      "Validation Loss: 0.0523, Validation Accuracy: 98.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transfer_Learning Epoch 15/20 [Train]: 100%|██████████| 2175/2175 [04:47<00:00,  7.56it/s, loss=0.3270, acc=93.97%]\n",
      "Transfer_Learning Epoch 15/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.67it/s, loss=0.3506]\n",
      "/tmp/ipykernel_23/582377012.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transfer_Learning Epoch 15/20:\n",
      "Training Loss: 0.1808, Training Accuracy: 93.97%\n",
      "Validation Loss: 0.0515, Validation Accuracy: 98.37%\n",
      "\n",
      "Early stopping triggered after 3 epochs without improvement\n",
      "\n",
      " Phase 2: Fine-tuning - Complete model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 1/20 [Train]: 100%|██████████| 2175/2175 [05:00<00:00,  7.23it/s, loss=0.0446, acc=95.77%]\n",
      "Fine_Tuning Epoch 1/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.53it/s, loss=0.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 1/20:\n",
      "Training Loss: 0.1239, Training Accuracy: 95.77%\n",
      "Validation Loss: 0.0241, Validation Accuracy: 99.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 2/20 [Train]: 100%|██████████| 2175/2175 [05:02<00:00,  7.19it/s, loss=0.0673, acc=97.25%]\n",
      "Fine_Tuning Epoch 2/20 [Val]: 100%|██████████| 272/272 [00:34<00:00,  7.84it/s, loss=0.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 2/20:\n",
      "Training Loss: 0.0813, Training Accuracy: 97.25%\n",
      "Validation Loss: 0.0188, Validation Accuracy: 99.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 3/20 [Train]: 100%|██████████| 2175/2175 [04:59<00:00,  7.25it/s, loss=0.0210, acc=97.92%]\n",
      "Fine_Tuning Epoch 3/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.43it/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 3/20:\n",
      "Training Loss: 0.0636, Training Accuracy: 97.92%\n",
      "Validation Loss: 0.0139, Validation Accuracy: 99.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 4/20 [Train]: 100%|██████████| 2175/2175 [05:11<00:00,  6.99it/s, loss=0.0308, acc=98.33%]\n",
      "Fine_Tuning Epoch 4/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.40it/s, loss=0.0049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 4/20:\n",
      "Training Loss: 0.0526, Training Accuracy: 98.33%\n",
      "Validation Loss: 0.0101, Validation Accuracy: 99.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 5/20 [Train]: 100%|██████████| 2175/2175 [05:18<00:00,  6.84it/s, loss=0.0254, acc=98.67%]\n",
      "Fine_Tuning Epoch 5/20 [Val]: 100%|██████████| 272/272 [00:37<00:00,  7.33it/s, loss=0.0018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 5/20:\n",
      "Training Loss: 0.0424, Training Accuracy: 98.67%\n",
      "Validation Loss: 0.0083, Validation Accuracy: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 6/20 [Train]: 100%|██████████| 2175/2175 [05:22<00:00,  6.75it/s, loss=0.0249, acc=98.78%]\n",
      "Fine_Tuning Epoch 6/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.59it/s, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 6/20:\n",
      "Training Loss: 0.0386, Training Accuracy: 98.78%\n",
      "Validation Loss: 0.0071, Validation Accuracy: 99.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 7/20 [Train]: 100%|██████████| 2175/2175 [05:10<00:00,  7.00it/s, loss=0.0103, acc=99.01%]\n",
      "Fine_Tuning Epoch 7/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.45it/s, loss=0.0023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 7/20:\n",
      "Training Loss: 0.0326, Training Accuracy: 99.01%\n",
      "Validation Loss: 0.0057, Validation Accuracy: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 8/20 [Train]: 100%|██████████| 2175/2175 [05:12<00:00,  6.95it/s, loss=0.0023, acc=99.09%]\n",
      "Fine_Tuning Epoch 8/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.75it/s, loss=0.0034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 8/20:\n",
      "Training Loss: 0.0297, Training Accuracy: 99.09%\n",
      "Validation Loss: 0.0044, Validation Accuracy: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 9/20 [Train]: 100%|██████████| 2175/2175 [05:06<00:00,  7.09it/s, loss=0.0079, acc=99.23%]\n",
      "Fine_Tuning Epoch 9/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.44it/s, loss=0.0018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 9/20:\n",
      "Training Loss: 0.0255, Training Accuracy: 99.23%\n",
      "Validation Loss: 0.0043, Validation Accuracy: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 10/20 [Train]: 100%|██████████| 2175/2175 [05:01<00:00,  7.21it/s, loss=0.0013, acc=99.31%]\n",
      "Fine_Tuning Epoch 10/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.66it/s, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine_Tuning Epoch 10/20:\n",
      "Training Loss: 0.0236, Training Accuracy: 99.31%\n",
      "Validation Loss: 0.0045, Validation Accuracy: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 11/20 [Train]: 100%|██████████| 2175/2175 [05:04<00:00,  7.14it/s, loss=0.0040, acc=99.36%]\n",
      "Fine_Tuning Epoch 11/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.45it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 11/20:\n",
      "Training Loss: 0.0216, Training Accuracy: 99.36%\n",
      "Validation Loss: 0.0033, Validation Accuracy: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 12/20 [Train]: 100%|██████████| 2175/2175 [05:00<00:00,  7.24it/s, loss=0.0086, acc=99.41%]\n",
      "Fine_Tuning Epoch 12/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.69it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine_Tuning Epoch 12/20:\n",
      "Training Loss: 0.0196, Training Accuracy: 99.41%\n",
      "Validation Loss: 0.0033, Validation Accuracy: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 13/20 [Train]: 100%|██████████| 2175/2175 [04:58<00:00,  7.28it/s, loss=0.0042, acc=99.43%]\n",
      "Fine_Tuning Epoch 13/20 [Val]: 100%|██████████| 272/272 [00:33<00:00,  8.03it/s, loss=0.0025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 13/20:\n",
      "Training Loss: 0.0184, Training Accuracy: 99.43%\n",
      "Validation Loss: 0.0029, Validation Accuracy: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 14/20 [Train]: 100%|██████████| 2175/2175 [05:00<00:00,  7.24it/s, loss=0.0025, acc=99.55%]\n",
      "Fine_Tuning Epoch 14/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.70it/s, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando el mejor modelo en results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth\n",
      "\n",
      "Fine_Tuning Epoch 14/20:\n",
      "Training Loss: 0.0163, Training Accuracy: 99.55%\n",
      "Validation Loss: 0.0020, Validation Accuracy: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 15/20 [Train]: 100%|██████████| 2175/2175 [04:57<00:00,  7.30it/s, loss=0.0010, acc=99.55%]\n",
      "Fine_Tuning Epoch 15/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.68it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine_Tuning Epoch 15/20:\n",
      "Training Loss: 0.0153, Training Accuracy: 99.55%\n",
      "Validation Loss: 0.0034, Validation Accuracy: 99.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 16/20 [Train]: 100%|██████████| 2175/2175 [04:57<00:00,  7.31it/s, loss=0.0122, acc=99.55%]\n",
      "Fine_Tuning Epoch 16/20 [Val]: 100%|██████████| 272/272 [00:36<00:00,  7.45it/s, loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine_Tuning Epoch 16/20:\n",
      "Training Loss: 0.0149, Training Accuracy: 99.55%\n",
      "Validation Loss: 0.0020, Validation Accuracy: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine_Tuning Epoch 17/20 [Train]: 100%|██████████| 2175/2175 [04:56<00:00,  7.33it/s, loss=0.0241, acc=99.61%]\n",
      "Fine_Tuning Epoch 17/20 [Val]: 100%|██████████| 272/272 [00:35<00:00,  7.72it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine_Tuning Epoch 17/20:\n",
      "Training Loss: 0.0141, Training Accuracy: 99.61%\n",
      "Validation Loss: 0.0022, Validation Accuracy: 99.95%\n",
      "\n",
      "Early stopping triggered after 3 epochs without improvement\n",
      "\n",
      "Realizando evaluación final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando: 100%|██████████| 272/272 [00:42<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen Final del Entrenamiento:\n",
      "Precisión en test: 99.87%\n",
      "Pérdida en test: 0.0041\n",
      "\n",
      "Métricas por clase:\n",
      "              precision    recall  f1-score      support\n",
      "A              1.000000  1.000000  1.000000   305.000000\n",
      "B              1.000000  1.000000  1.000000   270.000000\n",
      "C              1.000000  1.000000  1.000000   286.000000\n",
      "D              1.000000  1.000000  1.000000   270.000000\n",
      "E              1.000000  1.000000  1.000000   309.000000\n",
      "F              1.000000  1.000000  1.000000   286.000000\n",
      "G              1.000000  1.000000  1.000000   316.000000\n",
      "H              1.000000  1.000000  1.000000   300.000000\n",
      "I              1.000000  0.996454  0.998224   282.000000\n",
      "J              1.000000  1.000000  1.000000   278.000000\n",
      "K              0.993485  1.000000  0.996732   305.000000\n",
      "L              1.000000  1.000000  1.000000   298.000000\n",
      "M              1.000000  1.000000  1.000000   348.000000\n",
      "N              1.000000  1.000000  1.000000   312.000000\n",
      "O              1.000000  1.000000  1.000000   307.000000\n",
      "P              1.000000  0.996564  0.998279   291.000000\n",
      "Q              0.996241  1.000000  0.998117   265.000000\n",
      "R              0.996805  0.996805  0.996805   313.000000\n",
      "S              1.000000  1.000000  1.000000   285.000000\n",
      "T              1.000000  1.000000  1.000000   297.000000\n",
      "U              0.992857  0.992857  0.992857   280.000000\n",
      "V              0.996732  0.983871  0.990260   310.000000\n",
      "W              0.987055  0.996732  0.991870   306.000000\n",
      "X              1.000000  1.000000  1.000000   301.000000\n",
      "Y              1.000000  1.000000  1.000000   318.000000\n",
      "Z              1.000000  1.000000  1.000000   328.000000\n",
      "DEL            1.000000  1.000000  1.000000   290.000000\n",
      "NOTHING        1.000000  1.000000  1.000000   319.000000\n",
      "SPACE          1.000000  1.000000  1.000000   325.000000\n",
      "accuracy       0.998736  0.998736  0.998736     0.998736\n",
      "macro avg      0.998730  0.998734  0.998729  8700.000000\n",
      "weighted avg   0.998741  0.998736  0.998735  8700.000000\n",
      "\n",
      "Resultados guardados en: results/mobilenet_v2_data2/evaluation_20241207_144721/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "\n",
    "model, history_data = train_model_complete(\n",
    "\n",
    "    df=df,\n",
    "\n",
    "    transform=transform,\n",
    "\n",
    "    num_classes=PARAMS['num_classes'],\n",
    "\n",
    "    batch_size=PARAMS['batch_size'],\n",
    "\n",
    "    num_epochs1=PARAMS['epochs_phase1'],\n",
    "\n",
    "    num_epochs2=PARAMS['epochs_phase2'],\n",
    "\n",
    "    learning_rate1=PARAMS['learning_rate1'],\n",
    "\n",
    "    learning_rate2=PARAMS['learning_rate2'],\n",
    "\n",
    "    early_stopping_patience=PARAMS['early_stopping_patience'],\n",
    "\n",
    "    output_dir=OUTPUT_PATH\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688149a",
   "metadata": {
    "papermill": {
     "duration": 7.615259,
     "end_time": "2024-12-07T17:47:42.566505",
     "exception": false,
     "start_time": "2024-12-07T17:47:34.951246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a15f9",
   "metadata": {
    "papermill": {
     "duration": 7.563679,
     "end_time": "2024-12-07T17:47:57.606536",
     "exception": false,
     "start_time": "2024-12-07T17:47:50.042857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Modelo | Características de Salida (num_features) | Tamaño Mínimo de Entrada | Tamaño Recomendado |\n",
    "\n",
    "|--------|----------------------------------------|-------------------------|-------------------|\n",
    "\n",
    "| AlexNet | 4096 | 63x63 | 224x224 |\n",
    "\n",
    "| ConvNeXt-Tiny | 768 | 32x32 | 224x224 |\n",
    "\n",
    "| DenseNet121 | 1024 | 29x29 | 224x224 |\n",
    "\n",
    "| EfficientNet-B0 | 1280 | 32x32 | 224x224 |\n",
    "\n",
    "| EfficientNetV2-S | 1280 | 32x32 | 384x384 | \n",
    "\n",
    "| GoogLeNet | 1024 | 29x29 | 224x224 |\n",
    "\n",
    "| Inception V3 | 2048 | 75x75 | 299x299 |\n",
    "\n",
    "| MobileNetV2 | 1280 | 32x32 | 224x224 |\n",
    "\n",
    "| MobileNetV3-Large | 1280 | 32x32 | 224x224 |\n",
    "\n",
    "| ResNet50 | 2048 | 32x32 | 224x224 |\n",
    "\n",
    "| ResNeXt50 | 2048 | 32x32 | 224x224 |\n",
    "\n",
    "| VGG16 | 4096 | 32x32 | 224x224 |\n",
    "\n",
    "| ViT (Vision Transformer) | 768 | 32x32 | 224x224 |\n",
    "\n",
    "| Swin-Tiny | 768 | 32x32 | 224x224 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96edbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:48:12.791895Z",
     "iopub.status.busy": "2024-12-07T17:48:12.791166Z",
     "iopub.status.idle": "2024-12-07T17:48:12.796284Z",
     "shell.execute_reply": "2024-12-07T17:48:12.795454Z"
    },
    "papermill": {
     "duration": 7.63422,
     "end_time": "2024-12-07T17:48:12.798117",
     "exception": false,
     "start_time": "2024-12-07T17:48:05.163897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to print the number of features for a given model\n",
    "\n",
    "# def print_model_features(model_name):\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         # Crear instancia del modelo con los pesos correctos\n",
    "\n",
    "#         if model_name == 'mobilenet_v2':\n",
    "\n",
    "#             model = getattr(models, model_name)(weights=models.mobilenet_v2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         elif model_name == 'resnet50':\n",
    "\n",
    "#             model = getattr(models, model_name)(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             # Intenta cargar el modelo con pesos IMAGENET1K_V1\n",
    "\n",
    "#             model = getattr(models, model_name)(weights='IMAGENET1K_V1')\n",
    "\n",
    "            \n",
    "\n",
    "#         # Obtener número de características\n",
    "\n",
    "#         if hasattr(model, 'fc'):\n",
    "\n",
    "#             num_features = model.fc.in_features\n",
    "\n",
    "#         elif hasattr(model, 'classifier'):\n",
    "\n",
    "#             if isinstance(model.classifier, nn.Sequential):\n",
    "\n",
    "#                 num_features = model.classifier[-1].in_features  # Última capa en Sequential\n",
    "\n",
    "#             else:\n",
    "\n",
    "#                 num_features = model.classifier.in_features\n",
    "\n",
    "#         elif hasattr(model, 'head'):\n",
    "\n",
    "#             num_features = model.head.in_features\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             num_features = None  # Si no se encuentra un atributo compatible\n",
    "\n",
    "#             print(f\"Warning: Could not determine the number of features for {model_name}.\")\n",
    "\n",
    "            \n",
    "\n",
    "#         if num_features is not None:\n",
    "\n",
    "#             print(f\"{model_name}: {num_features} features\")\n",
    "\n",
    "\n",
    "\n",
    "#     except Exception as e:\n",
    "\n",
    "#         print(f\"Error with {model_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Ejemplo de uso\n",
    "\n",
    "# print_model_features('mobilenet_v2')\n",
    "\n",
    "# print_model_features('resnet50')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 23079,
     "sourceId": 29550,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6240244,
     "sourceId": 10114306,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6240257,
     "sourceId": 10114332,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11419.809919,
   "end_time": "2024-12-07T17:48:21.938693",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-07T14:38:02.128774",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
