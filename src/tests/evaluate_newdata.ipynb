{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thop\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics #conda install -c conda-forge torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import torchviz\n",
    "import graphviz\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'data1'\n",
    "\n",
    "if DATA_TYPE == 'data2':\n",
    "    DATA_PATH = os.path.join('..', 'data', 'ASL','asl_alphabet_train')\n",
    "elif DATA_TYPE == 'data3':\n",
    "    DATA_PATH = os.path.join('..', 'data', 'unified_data','unified_data')\n",
    "elif DATA_TYPE == 'data1':\n",
    "    DATA_PATH = os.path.join('..', 'data', 'webcam_data','unified_data')\n",
    "else:\n",
    "    raise ValueError(f\"Data {DATA_TYPE} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\webcam_data\\\\unified_data'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando carpetas: 100%|██████████| 29/29 [00:25<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen del Dataset:\n",
      "Total de imágenes: 11600\n",
      "Número de clases: 29\n",
      "\n",
      "Distribución de clases:\n",
      "Labels\n",
      "0     400\n",
      "1     400\n",
      "10    400\n",
      "11    400\n",
      "12    400\n",
      "13    400\n",
      "14    400\n",
      "15    400\n",
      "16    400\n",
      "17    400\n",
      "18    400\n",
      "19    400\n",
      "2     400\n",
      "20    400\n",
      "21    400\n",
      "22    400\n",
      "23    400\n",
      "24    400\n",
      "25    400\n",
      "26    400\n",
      "27    400\n",
      "28    400\n",
      "3     400\n",
      "4     400\n",
      "5     400\n",
      "6     400\n",
      "7     400\n",
      "8     400\n",
      "9     400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mínimo de muestras por clase: 400\n",
      "Máximo de muestras por clase: 400\n",
      "\n",
      "Tamaños de imagen:\n",
      "Mínimo: [640 480]\n",
      "Máximo: [640 480]\n",
      "Moda: [640 480]\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "                                      Filepaths Labels  Label_idx  Image_size\n",
      "0  ..\\data\\webcam_data\\unified_data\\0\\0 (2).jpg      0          0  (640, 480)\n",
      "1  ..\\data\\webcam_data\\unified_data\\0\\0 (3).jpg      0          0  (640, 480)\n",
      "2  ..\\data\\webcam_data\\unified_data\\0\\0 (4).jpg      0          0  (640, 480)\n",
      "3      ..\\data\\webcam_data\\unified_data\\0\\0.jpg      0          0  (640, 480)\n",
      "4  ..\\data\\webcam_data\\unified_data\\0\\1 (2).jpg      0          0  (640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to create the DataFrame from the dataset\n",
    "# Uncomment to use. The output is a dataframe stored in asl_dataset_info.csv\n",
    "\n",
    "def create_dataframe(data_path):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con las rutas de las imágenes y sus etiquetas.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Ruta al directorio que contiene las carpetas de clases (0-29)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columnas ['Filepaths', 'Labels', 'Label_idx']\n",
    "    \"\"\"\n",
    "    # Convertir a Path object para mejor manejo de rutas\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise ValueError(f\"El directorio {data_path} no existe\")\n",
    "    \n",
    "    # Listas para almacenar datos\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    label_indices = []\n",
    "    img_sizes = []\n",
    "    \n",
    "    # Obtener todas las carpetas y ordenarlas numéricamente\n",
    "    folders = sorted([f for f in data_path.iterdir() if f.is_dir()], \n",
    "                    key=lambda x: int(x.name))\n",
    "    \n",
    "    print(\"Creando DataFrame...\")\n",
    "    # Usar tqdm para mostrar progreso\n",
    "    for folder in tqdm(folders, desc=\"Procesando carpetas\"):\n",
    "        label_idx = int(folder.name)\n",
    "        \n",
    "        # Obtener todas las imágenes en la carpeta\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "        images = [f for f in folder.iterdir() \n",
    "                 if f.suffix.lower() in valid_extensions]\n",
    "        \n",
    "        for img_path in images:\n",
    "            # Verificar que la imagen se puede leer\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"Advertencia: No se pudo leer {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                height, width = img.shape[:2]\n",
    "                \n",
    "                filepaths.append(str(img_path))\n",
    "                labels.append(folder.name)\n",
    "                label_indices.append(label_idx)\n",
    "                img_sizes.append((width, height))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {img_path}: {str(e)}\")\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Filepaths': filepaths,\n",
    "        'Labels': labels,\n",
    "        'Label_idx': label_indices,\n",
    "        'Image_size': img_sizes\n",
    "    })\n",
    "    \n",
    "    # Mostrar información del dataset\n",
    "    print(\"\\nResumen del Dataset:\")\n",
    "    print(f\"Total de imágenes: {len(df)}\")\n",
    "    print(f\"Número de clases: {len(df['Labels'].unique())}\")\n",
    "    print(\"\\nDistribución de clases:\")\n",
    "    print(df['Labels'].value_counts().sort_index())\n",
    "    \n",
    "    # Verificar balance de clases\n",
    "    min_samples = df['Labels'].value_counts().min()\n",
    "    max_samples = df['Labels'].value_counts().max()\n",
    "    print(f\"\\nMínimo de muestras por clase: {min_samples}\")\n",
    "    print(f\"Máximo de muestras por clase: {max_samples}\")\n",
    "    \n",
    "    # Verificar tamaños de imagen\n",
    "    sizes = pd.DataFrame(df['Image_size'].tolist(), columns=['width', 'height'])\n",
    "    print(\"\\nTamaños de imagen:\")\n",
    "    print(f\"Mínimo: {sizes.min().values}\")\n",
    "    print(f\"Máximo: {sizes.max().values}\")\n",
    "    print(f\"Moda: {sizes.mode().iloc[0].values}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "try:\n",
    "    # Images in 'data/...'  \n",
    "    df = create_dataframe(DATA_PATH)\n",
    "    \n",
    "    # Save dataframe of images paths and labels\n",
    "    if DATA_TYPE == 'data2':\n",
    "        df.to_csv('asl_dataset_info.csv', index=False)\n",
    "    elif DATA_TYPE == 'data3':\n",
    "        df.to_csv('unified_data_dataset_info.csv', index=False)\n",
    "    elif DATA_TYPE == 'data1':\n",
    "        df.to_csv('unified_webcam_dataset_info.csv', index=False)\n",
    "\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Filepaths  Labels  Label_idx  Image_size\n",
      "0  ..\\data\\webcam_data\\unified_data\\0\\0 (2).jpg       0          0  (640, 480)\n",
      "1  ..\\data\\webcam_data\\unified_data\\0\\0 (3).jpg       0          0  (640, 480)\n",
      "2  ..\\data\\webcam_data\\unified_data\\0\\0 (4).jpg       0          0  (640, 480)\n",
      "3      ..\\data\\webcam_data\\unified_data\\0\\0.jpg       0          0  (640, 480)\n",
      "4  ..\\data\\webcam_data\\unified_data\\0\\1 (2).jpg       0          0  (640, 480)\n"
     ]
    }
   ],
   "source": [
    "if DATA_TYPE == 'data2':\n",
    "    df = pd.read_csv('asl_dataset_info.csv')\n",
    "elif DATA_TYPE == 'data3':\n",
    "    df = pd.read_csv('unified_data_dataset_info.csv')\n",
    "elif DATA_TYPE == 'data1':\n",
    "    df = pd.read_csv('unified_webcam_dataset_info.csv')\n",
    "else:\n",
    "    raise ValueError(f\"Data {DATA_TYPE} not found.\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "GPU memory available: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Configure the device for training\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        torch.backends.cudnn.benchmark = True  # Optimiza el rendimiento\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"GPU not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "DEVICE = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading ASL (American Sign Language) images.\n",
    "    \n",
    "    This dataset class handles loading and preprocessing of ASL hand gesture images.\n",
    "    It supports on-the-fly data augmentation and preprocessing for model training.\n",
    "    \n",
    "    Attributes:\n",
    "        df (pd.DataFrame): DataFrame containing image paths and labels\n",
    "        transform (callable): Torchvision transforms for image preprocessing\n",
    "        is_training (bool): Flag to enable/disable data augmentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the ASL Dataset.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with columns ['Filepaths', 'Labels']\n",
    "            transform (callable, optional): Transform to be applied to images\n",
    "            is_training (bool): If True, enables data augmentation\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.labels = pd.Categorical(dataframe['Labels']).codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of images in the dataset.\"\"\"\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch and preprocess a single image item from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image to fetch\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label) where image is the preprocessed tensor\n",
    "                  and label is the corresponding class index\n",
    "        \"\"\"\n",
    "        img_path = self.dataframe.iloc[idx]['Filepaths']\n",
    "        label = self.dataframe.iloc[idx]['Label_idx']  # Asegúrate de que esto sea un número entero\n",
    "        \n",
    "        try:\n",
    "            # Read and preprocess image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Image not found: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Defining data type for labels\n",
    "            label = torch.tensor(int(label), dtype=torch.long)\n",
    "            \n",
    "            return image, label\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading imagen {img_path}: {str(e)}\")\n",
    "            # Retorning a black image\n",
    "            if self.transform:\n",
    "                dummy_image = torch.zeros((3, 384, 384))\n",
    "            else:\n",
    "                dummy_image = np.zeros((384, 384, 3))\n",
    "            return dummy_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_loaders(df, transforms=None, batch_size=32, train_split=0.8, val_split=0.1):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing image paths and labels\n",
    "        transform (callable): Torchvision transforms for image preprocessing\n",
    "        batch_size (int): Batch size for data loaders\n",
    "        train_split (float): Proportion of data used for training (default: 0.8)\n",
    "        val_split (float): Proportion of data used for validation (default: 0.1)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    dataset = ASLDataset(df, transform=transforms)\n",
    "\n",
    "    # Calculate sizes\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = int(val_split * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Create splits\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Adjust workers according to your CPU cores (generally num_cores - 1)\n",
    "    # num_workers = min(4, os.cpu_count() - 1) if os.cpu_count() > 1 else 0\n",
    "\n",
    "    # Configure a common DataLoader for training, validation, and testing dataloaders\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 0,#num_workers\n",
    "        'pin_memory': torch.cuda.is_available(),\n",
    "        'persistent_workers': False#if num_workers > 0 else False\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **dataloader_kwargs)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, **dataloader_kwargs)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_metrics(model, test_loader, evaluation_path):\n",
    "    \"\"\"\n",
    "    Generate and save comprehensive evaluation metrics for the model.\n",
    "    \n",
    "    This function creates various visualizations and metrics including:\n",
    "    - Training/validation loss curves\n",
    "    - Accuracy plots\n",
    "    - Confusion matrix\n",
    "    - Classification report\n",
    "    - Per-class performance metrics\n",
    "    \n",
    "    Evaluation Components:\n",
    "    ---------------------\n",
    "    1. Model Performance Metrics:\n",
    "        - Test Loss (Cross-Entropy)\n",
    "        - Test Accuracy\n",
    "        - Per-class Precision, Recall, and F1-score\n",
    "    \n",
    "    2. Visualizations:\n",
    "        - Confusion Matrix: Shows prediction patterns across all classes\n",
    "        - Training History Plots:\n",
    "            * Loss curves (training and validation)\n",
    "            * Accuracy curves (training and validation)\n",
    "    \n",
    "    3. Saved Outputs:\n",
    "        - classification_metrics.csv: Detailed per-class metrics\n",
    "        - training_history.json: Complete training history\n",
    "        - confusion_matrix.png: Visual representation of model predictions\n",
    "        - training_curves.png: Learning curves from both training phases\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model to evaluate\n",
    "        test_loader (DataLoader): DataLoader for test data\n",
    "        history_phase1 (dict): Training history from phase 1\n",
    "        history_phase2 (dict): Training history from phase 2\n",
    "        evaluation_path (str): Directory to save evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing all evaluation metrics and history:\n",
    "            {\n",
    "                'training_history': {\n",
    "                    'phase1': {train_losses, train_accuracies, val_losses, val_accuracies},\n",
    "                    'phase2': {train_losses, train_accuracies, val_losses, val_accuracies}\n",
    "                },\n",
    "                'final_metrics': {\n",
    "                    'test_loss': float,\n",
    "                    'test_accuracy': float,\n",
    "                    'classification_report': dict\n",
    "                }\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluando'):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE, dtype=torch.long)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.float()\n",
    "            \n",
    "            # Loss function using cross entropy \n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate final metrics \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Load a label mapping \n",
    "    label_mapping = {\n",
    "        0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\", \n",
    "        5: \"F\", 6: \"G\", 7: \"H\", 8: \"I\", 9: \"J\", \n",
    "        10: \"K\", 11: \"L\", 12: \"M\", 13: \"N\", 14: \"O\", \n",
    "        15: \"P\", 16: \"Q\", 17: \"R\", 18: \"S\", 19: \"T\", \n",
    "        20: \"U\", 21: \"V\", 22: \"W\", 23: \"X\", 24: \"Y\", \n",
    "        25: \"Z\", 26: \"DEL\", 27: \"NOTHING\", 28: \"SPACE\"}\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    classification_rep = classification_report(all_labels, all_preds, \n",
    "                                            target_names=list(label_mapping.values()),\n",
    "                                            output_dict=True)\n",
    "    \n",
    "    # Save visualizations\n",
    "    _save_confusion_matrix(cm, label_mapping, evaluation_path)\n",
    "    \n",
    "    # Prepare complete history\n",
    "    history_data = {\n",
    "        'final_metrics': {\n",
    "            'test_loss': float(test_loss),\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'classification_report': classification_rep\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save history in a JSON\n",
    "    with open(os.path.join(evaluation_path, 'test_results_extra.json'), 'w') as f:\n",
    "        json.dump(history_data, f, indent=4)\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nResumen Final del Entrenamiento:\")\n",
    "    print(f\"Precisión en test: {test_accuracy:.2f}%\")\n",
    "    print(f\"Pérdida en test: {test_loss:.4f}\")\n",
    "    \n",
    "    return history_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config (option):\n",
    "    model_configs = {\n",
    "        1: {\n",
    "            'model_type': 'efficientnet',\n",
    "            'evaluation_path': '../results/efficientnet_v2_s_data2/evaluation_20241206_174129',\n",
    "            'model_path': '../results/efficientnet_v2_s_data2/evaluation_20241206_174129/checkpoints/Fine_Tuning_best_model.pth'\n",
    "        },\n",
    "        2: {\n",
    "            'model_type': 'efficientnet',\n",
    "            'evaluation_path': '../results/efficientnet_v2_s_data3/evaluation_20241206_152358',\n",
    "            'model_path': '../results/efficientnet_v2_s_data3/evaluation_20241206_152358/checkpoints/Fine_Tuning_best_model.pth'\n",
    "        },\n",
    "        3: {\n",
    "            'model_type': 'mobilenet',\n",
    "            'evaluation_path': '../results/mobilenet_v2_data2/evaluation_20241207_144721',\n",
    "            'model_path': '../results/mobilenet_v2_data2/evaluation_20241207_144721/checkpoints/Fine_Tuning_best_model.pth'\n",
    "        },\n",
    "        4: {\n",
    "            'model_type': 'mobilenet',\n",
    "            'evaluation_path': '../results/mobilenet_v2_data3/evaluation_20241207_112444',\n",
    "            'model_path': '../results/mobilenet_v2_data3/evaluation_20241207_112444/checkpoints/Fine_Tuning_best_model.pth'\n",
    "        }\n",
    "    }\n",
    "    return model_configs[option]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_confusion_matrix(cm, label_mapping, evaluation_path):\n",
    "    \"\"\"Save confusion matrix of model evaluation\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(label_mapping.values()),\n",
    "                yticklabels=list(label_mapping.values()))\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Valor Real')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(evaluation_path, 'confusion_matrix_extra.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type,model_path):\n",
    "        \"\"\"Load and prepare model based on type.\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        try:\n",
    "            # For PyTorch models\n",
    "            if model_type == 'efficientnet':\n",
    "                from efficientNet.model_class import ASLModel\n",
    "                model = ASLModel(num_classes=29)\n",
    "            elif model_type == 'mobilenet':\n",
    "                from mobileNet.model_class import ASLModel\n",
    "                model = ASLModel(num_classes=29)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "            \n",
    "            checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "            model.eval().to(device)\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating  data loaders...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating  data loaders...\")\n",
    "train_loader, val_loader, test_loader = create_test_loaders(\n",
    "    df=df,\n",
    "    transforms = transform,\n",
    "    batch_size=32,\n",
    "    train_split=0.1,\n",
    "    val_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = get_model_config(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_config['model_type'],model_config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando: 100%|██████████| 290/290 [01:24<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen Final del Entrenamiento:\n",
      "Precisión en test: 10.94%\n",
      "Pérdida en test: 5.9131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_metrics': {'test_loss': 5.913103194894462,\n",
       "  'test_accuracy': 10.9375,\n",
       "  'classification_report': {'A': {'precision': 0.0711864406779661,\n",
       "    'recall': 0.13548387096774195,\n",
       "    'f1-score': 0.09333333333333334,\n",
       "    'support': 310.0},\n",
       "   'B': {'precision': 0.09131205673758866,\n",
       "    'recall': 0.6338461538461538,\n",
       "    'f1-score': 0.15962805114296785,\n",
       "    'support': 325.0},\n",
       "   'C': {'precision': 0.22448979591836735,\n",
       "    'recall': 0.07028753993610223,\n",
       "    'f1-score': 0.1070559610705596,\n",
       "    'support': 313.0},\n",
       "   'D': {'precision': 0.09803921568627451,\n",
       "    'recall': 0.015197568389057751,\n",
       "    'f1-score': 0.02631578947368421,\n",
       "    'support': 329.0},\n",
       "   'E': {'precision': 0.08295625942684766,\n",
       "    'recall': 0.16467065868263472,\n",
       "    'f1-score': 0.11033099297893681,\n",
       "    'support': 334.0},\n",
       "   'F': {'precision': 0.0684931506849315,\n",
       "    'recall': 0.01557632398753894,\n",
       "    'f1-score': 0.025380710659898477,\n",
       "    'support': 321.0},\n",
       "   'G': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 325.0},\n",
       "   'H': {'precision': 0.2903225806451613,\n",
       "    'recall': 0.02786377708978328,\n",
       "    'f1-score': 0.05084745762711865,\n",
       "    'support': 323.0},\n",
       "   'I': {'precision': 0.08333333333333333,\n",
       "    'recall': 0.0030864197530864196,\n",
       "    'f1-score': 0.005952380952380952,\n",
       "    'support': 324.0},\n",
       "   'J': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 313.0},\n",
       "   'K': {'precision': 0.06060606060606061,\n",
       "    'recall': 0.006269592476489028,\n",
       "    'f1-score': 0.011363636363636364,\n",
       "    'support': 319.0},\n",
       "   'L': {'precision': 0.42105263157894735,\n",
       "    'recall': 0.10191082802547771,\n",
       "    'f1-score': 0.1641025641025641,\n",
       "    'support': 314.0},\n",
       "   'M': {'precision': 0.05166051660516605,\n",
       "    'recall': 0.04430379746835443,\n",
       "    'f1-score': 0.04770017035775128,\n",
       "    'support': 316.0},\n",
       "   'N': {'precision': 0.0625,\n",
       "    'recall': 0.0064516129032258064,\n",
       "    'f1-score': 0.011695906432748537,\n",
       "    'support': 310.0},\n",
       "   'O': {'precision': 0.11044417767106843,\n",
       "    'recall': 0.28134556574923547,\n",
       "    'f1-score': 0.15862068965517243,\n",
       "    'support': 327.0},\n",
       "   'P': {'precision': 0.038461538461538464,\n",
       "    'recall': 0.0031446540880503146,\n",
       "    'f1-score': 0.005813953488372093,\n",
       "    'support': 318.0},\n",
       "   'Q': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 317.0},\n",
       "   'R': {'precision': 0.25,\n",
       "    'recall': 0.03459119496855346,\n",
       "    'f1-score': 0.06077348066298342,\n",
       "    'support': 318.0},\n",
       "   'S': {'precision': 0.09302325581395349,\n",
       "    'recall': 0.025236593059936908,\n",
       "    'f1-score': 0.03970223325062035,\n",
       "    'support': 317.0},\n",
       "   'T': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 320.0},\n",
       "   'U': {'precision': 0.06847935548841894,\n",
       "    'recall': 0.2085889570552147,\n",
       "    'f1-score': 0.10310841546626232,\n",
       "    'support': 326.0},\n",
       "   'V': {'precision': 0.04040404040404041,\n",
       "    'recall': 0.012345679012345678,\n",
       "    'f1-score': 0.018912529550827423,\n",
       "    'support': 324.0},\n",
       "   'W': {'precision': 0.10298707088720464,\n",
       "    'recall': 0.721875,\n",
       "    'f1-score': 0.18025751072961374,\n",
       "    'support': 320.0},\n",
       "   'X': {'precision': 0.56,\n",
       "    'recall': 0.13003095975232198,\n",
       "    'f1-score': 0.21105527638190955,\n",
       "    'support': 323.0},\n",
       "   'Y': {'precision': 0.14666666666666667,\n",
       "    'recall': 0.034161490683229816,\n",
       "    'f1-score': 0.055415617128463476,\n",
       "    'support': 322.0},\n",
       "   'Z': {'precision': 0.14556962025316456,\n",
       "    'recall': 0.06948640483383686,\n",
       "    'f1-score': 0.09406952965235174,\n",
       "    'support': 331.0},\n",
       "   'DEL': {'precision': 0.3357664233576642,\n",
       "    'recall': 0.14511041009463724,\n",
       "    'f1-score': 0.2026431718061674,\n",
       "    'support': 317.0},\n",
       "   'NOTHING': {'precision': 0.50625,\n",
       "    'recall': 0.25471698113207547,\n",
       "    'f1-score': 0.3389121338912134,\n",
       "    'support': 318.0},\n",
       "   'SPACE': {'precision': 0.10526315789473684,\n",
       "    'recall': 0.006535947712418301,\n",
       "    'f1-score': 0.012307692307692308,\n",
       "    'support': 306.0},\n",
       "   'accuracy': 0.109375,\n",
       "   'macro avg': {'precision': 0.1416988740965207,\n",
       "    'recall': 0.10869372350577593,\n",
       "    'f1-score': 0.07914824787818034,\n",
       "    'support': 9280.0},\n",
       "   'weighted avg': {'precision': 0.1415525338548702,\n",
       "    'recall': 0.109375,\n",
       "    'f1-score': 0.07932798379704581,\n",
       "    'support': 9280.0}}}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_evaluation_metrics(model, test_loader, model_config['evaluation_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
